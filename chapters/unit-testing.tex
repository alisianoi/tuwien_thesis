\chapter{Unit Testing Frameworks}
\label{chapter:unit-testing-framework}

For the lyra2-java project testing is important. It makes sure that the new implementation produces the same results as the reference implementation. Formal specification and verification methods (for example, as described in \cite{lamsweerde:2000:formal-specification, mueller:1994:formal-specification}) would have been too complex in this situation.

A more practical verification approach is to provide reasonable unit test coverage for the new project, as suggested in \cite{williams:2010:unit-tests-rock}. However, this approach also provides its own classical questions: which tools to use \cite{daka:2014:unit-testing-tools} and when to stop writing tests \cite{elberzhager:2012:reducing-effort}.

Section \ref{sec:unit-terms-and-definitions} will introduce the basic practices and definitions. Section \ref{sec:unit-boost-google} will discuss how the two similar projects, the reference implementation of Lyra2 and of Argon2, address testing. Sections \ref{sec:unit-junit-testng} and \ref{sec:unit-pytest} motivate the choice of tools and techniques for the Java and Python projects.

\section{Terms and Definitions}
\label{sec:unit-terms-and-definitions}

\emph{Unit testing} is the process of verifying that the program produces expected results when given specific inputs. \emph{Unit under test} is a common term that refers to the specific part of the code that is being tested by a specific \emph{test case}. Unit testing is often powered by a \emph{unit testing framework} which is a set of tools dedicated to simplify both writing and running unit tests. \emph{Test Driven Development} is a software development practice which expects a piece of functionality to be produced together with the accompanying tests.

One notable property of a typical unit test is that it deals with the smallest logical piece of code possible: one particular function or a single method of a class. Consequently, distinct unit tests are ususally independent of each other and can be run in any order or in parallel. Parallelized unit test execution is therefore a desirable feature of a unit testing framework.

However, there are also cases when the complexity of a unit test is higher. For example, when a class heavily relies on data coming from a database, a unit test needs to \emph{mock} the database connection and the data. \emph{Mocking} is the process of simulating a real object with a simplified version of it.

It is often the case that many unit tests share the same logic. Specifically, when testing hash functions, this logic could be summarized with the following steps:

\begin{enumerate}
    \item Select a configuration of the hash function.
    \item Provide input data: a password, a salt, etc.
    \item Compute the hash value and compare it to the correct one.
   \end{enumerate}

Writing a unit test for each combination of the hash function configuration and each set of input parameters is a daunting task. \emph{Parametrized unit testing} allows to generate a template for a large number of unit tests and avoid the extra labour \cite{tillmann:2010:parametrized-unit-tests-rock}. Therefore, unit testing frameworks that support this particular feature are of special interest in this work.

\emph{Code coverage} indicates how well a program is tested. It is low when only a small portion of the program is tested, and high otherwise. There are many ways to measure code coverage, some of which are described in \cite{elberzhager:2012:reducing-effort}. In practice, the most common code coverage metric is the number of lines of code (LOCs) covered by the test suite as a percentage of the total number of LOCs.

Other types of coverage include: \emph{function} and \emph{statement} coverage (i.e. the portion of functions or statements that has been executed), \emph{branch} and \emph{condition} coverage (i.e. the portion of conditions/branches executed in relation to the total number of \emph{all possible} combinations) and many others. For the sake of simplicity, the projects developed in this work will not be using these metrics.

\emph{Continuous integration} (CI) is the practice of running unit tests (and measuring code coverage changes) with every codebase change. This approach helps identify problems early and fix them quicker \cite{williams:2010:unit-tests-rock}. Continuous integration often occurs transparenly on a separate set of dedicated machines and its status is visible to the developers at all times. Its popularity and utility is undisputable, with such services like TravisCI \cite{travis:2017:homepage}, AppVeyor \cite{appveyor:2017:homepage} and CircleCI \cite{circleci:2017:homepage} providing both commercial and free (for open source projects) continuous integration as a service.

\section{Choice of the Unit Testing Framework}
\label{sec:unit-choice}

Although the reference Lyra2 project in \cite{github:2017:lyra} has automated tests, it does not use a dedicated unit testing framework. This is a reasonable choice which has its advantages. In particular, it allows to keep the complexity of the project lower and not rely on external libraries. The build process is somewhat simplified as well.

Furthermore, when it comes to the \texttt{C/C++} ecosystem, the choice of the framework is not trivial. The authors of Argon2 have also faced this choice and opted out to write their own test harness in \cite{github:2017:argon2-test.c}. Section \ref{sec:unit-boost-google} will provide a plausible explanation. Section \ref{sec:unit-junit-testng} will provide motivation for my choice of the unit testing framework for the lyra2-java port and section \ref{sec:unit-pytest} will describe the test suite for the Python script harness.

\begin{table}
\begin{tabular}{llll}
    Name & License & Parametrization & Parallelism \\ \hline
\texttt{Boost.Test} & Boost Software License & yes & needs 3\textsuperscript{rd} party runner (\texttt{cmake}) \\
\texttt{JUnit4} & Eclipse Public License & yes & needs 3\textsuperscript{rd} party runner (\texttt{mvn}) \\
\texttt{py.test} & MIT & yes & needs 3\textsuperscript{rd} party plugin (\texttt{pytest.xdist})
% Google Test & MIT & parameters? & yes
\end{tabular}
\caption{Important features of several unit testing frameworks.}
\label{table:framework-features-cpp}
\end{table}

\subsection{\texttt{Boost.Test}}
\label{sec:unit-boost-google}

There is a large choice of unit testing frameworks for the \texttt{C/C++} ecosystem. An extensive list can be found in \cite{wiki:2017:frameworks-c, wiki:2017:frameworks-cpp}. Given the number of available solutions, their comparison and an educated choice would be a large and daunting task for any developer. This could be one of the reasons why project authors sometimes tend to avoid actually using a unit testing framework. Instead, it is sometimes more practical to write a separate test file that would run a few sanity checks.

Consider the case of Argon2. As one can see in \cite{github:2017:argon2}, the in-house \texttt{src/test.c} test program for Argon2 was introduced on 25\textsuperscript{th} of January 2016. The commit hash starts with \texttt{7450df88} and it is number 317 out of (current) almost 600 in the version control history. So it is safe to say that this testing was introduced at the later stages of the project when the need for it was apparent \footnote{https://github.com/P-H-C/phc-winner-argon2/issues/85}.

A similar story could be observed for the reference Lyra2 project \cite{github:2017:lyra}. The rest of this section will explain why a rather popular unit testing library \texttt{Boost.Test} was not used in either of the projects. This particular library was tried because of my personal developer preference. A different developer could attempt the same steps with another library, like \texttt{CTest} or \texttt{Google Test}.

\texttt{Boost} itself is a large collection of different libraries for \texttt{C++}. It was originally founded by Beman Dawes and David Abrahams but today the number of contributors is in the hundreds. The scope of the libraries spans from concurrent programming to regular expressions to linear algebra. There is a formal submission process \footnote{http://www.boost.org/development/submissions.html} for any library that would like to be included into the collection as well as rigorous review in the mailing lists \footnote{http://www.boost.org/community/groups.html\#main}. Many of the ideas that originate in \texttt{Boost} are later adopted by the \texttt{C++} language standard.

The \texttt{Boost.Test} is a unit testing framework that is part of \texttt{Boost}. It is licensed under the Boost Software License which is a free software license, OSI-approved and compatible with GPL. The library documnetation is located in \cite{boost:2017:test-docs}.

The main features of the library are summarized in table \ref{table:framework-features-cpp}. When it comes to password hashing, support for test parametrization becomes important because of the common test scenario described in Section \ref{sec:unit-terms-and-definitions}. \texttt{Boost.Test} provides all the common features a unit testing library is expected to have. The tests can be completely decoupled from the implementation, subdivided  into test suits and automatically registered with the test runner. The registration can be accomplished with a single \mintinline{cpp}{#include<...>} and a call to the \mintinline{cpp}{BOOST_AUTO_TEST_CASE} macro, as shown in \ref{figure:boost-auto-test-case}.

\begin{figure}
\centering
\begin{minted}{cpp}
#define BOOST_TEST_MODULE example_module_name
#include <boost/test/included/unit_test.hpp>

BOOST_AUTO_TEST_CASE(name_of_test_function) {
    BOOST_TEST(true);
}
  \end{minted}
  \caption{Automatic unit test registration with the \texttt{Boost.Test} framework}
  \label{figure:boost-auto-test-case}
  \end{figure}

Apart from that \texttt{Boost.Test} has a few convenience features. First of all, it was designed to be used as a header-only library. This means that the build process of a project needs only slight modification. Secondly, \texttt{Boost.Test} supports parametrized tests which are called "Data-Driven Test Cases" and can be found in \cite{boost:2017:test-data-driven}. First step is to declare such tests with the special \mintinline{cpp}{BOOST_DATA_TEST_CASE} macro. More details about its usage can be found in \cite{boost:2017:test-docs-data-macro}. The data also has to be wrapped into a dataset, the procedure is described in detail in \cite{boost:2017:test-docs-dataset}.

This section should have given the reader a hint that setting up \texttt{Boost.Test} for the real-world usage is not entirely painless. Even though the documentation provides an accurate description of the process, it still takes significant time to set everything up. Therefore it is understandable why Lyra2 and Argon2 projects chose to use their own testing solutions and not take advantage of test parametrization.

\subsection{\texttt{JUnit} and \texttt{TestNG}}
\label{sec:unit-junit-testng}

\begin{figure}
\small
\begin{minted}[linenos]{java}
@RunWith(Parameterized.class)
public class Lyra2Test {
    @Parameterized.Parameters
    public static Collection<Object[]> setupClass() {
        // Simplified initialization of a YAML data loader
        Yaml yaml = new Yaml();

        // A list of YAML file names with test vectors and resulting hashes
        String[] fnames = new String[] {"test-data-file_0.yml"};
        List<Object[]> entries = new ArrayList<>();

        for (String fname: fnames) {
            // Simplified loading of the YAML data from the file
            for (Object data : yaml.loadAll(reader)) {
                entries.add(new Object[]{data});
            }
        }

        return entries; // data provider has finished, returning result
    }

    private DataEntry entry;

    // Injection of test parameters
    public Lyra2Test(DataEntry entry) {
        this.entry = entry;
    }

    @Test
    public void simpleTest() {

        LyraParams params = new LyraParams(/* use entry to initialize */);

        byte[] hash = new byte[entry.klen];
        byte[] pass = entry.pass.getBytes();
        byte[] salt = entry.salt.getBytes();

        // Run the computation
        Lyra2.phs(hash, pass, salt, params);
        // Fetch the correct hash value
        byte[] correct_hash = pack.bytes(entry.hash);
        // Compare the computation result to the correct hash
        assertArrayEquals(correct_hash, hash);
    }
}
\end{minted}
\normalsize
\caption{A simplified example of lyra2-java parametrized testing with \texttt{JUnit4}.}
\label{fig:junit4-parametrization}
\end{figure}

The Java ecosystem offers \texttt{JUnit}, which is a unit testing framework inspired by \texttt{xUnit}. The latter is a collective name for a specific architecture introduced by \texttt{NUnit}. This architecture is described below.

The main element of \texttt{JUnit} is a \emph{test case} which is usually a class that contains testing logic. Such a class might have special methods called \emph{fixtures} which are responsible for setting up and tearing down the context in which a test is executed. For example, this might include repeatedly creating a set of objects or opening a database connection. The methods marked with the \mintinline{java}{@Before} (respectively, \mintinline{java}{@After}) annotation are executed before (respectively, after) each individual test. The \mintinline{java}{@BeforeClass} (respectively, \mintinline{java}{@AfterClass}) annotation ensures that a method is run exactly once before (respectively, after) the test class is instantiated.

Several test cases can be collected into a single \emph{test suite}. Finally, a program called a \emph{test runner} is responsible for discovering the test cases, running them and reporting the results back to the user. The test runner can be instructed to run (or skip) specific test suits.

As well as \texttt{JUnit}, the Java ecosystem has a unit testing framework called \texttt{TestNG} \cite{testng:2017:home}. There are several in-depth comparisons available online in \cite{mkyong:2017:testng-vs-junit, wiki:2017:testng-vs-junit}. Arguably the most important differences follow.

Firstly, both frameworks allow to implement parametrization tests but the implementation details are significantly different.  \texttt{JUnit4} requires the developer to mark the class with the \mintinline{java}{@RunWith} decorator as well as provide a class method marked with \mintinline{java}{@Parameters} and return a single \mintinline{java}{Collections<Object[]>} of data. On the other hand, \texttt{TestNG} has two distinct mechanisms, one of which uses \texttt{testng.xml} and the other is a \mintinline{java}{@DataProvider} method which returns either a single \mintinline{java}{Object[][]} or an \mintinline{java}{Iterator<Object[]>}. With the first mechanism, the test data is stored separately in \texttt{testng.xml} while the second approach allows to have a function that generates the data \footnote{http://testng.org/doc/documentation-main.html\#parameters}.

Formally, \texttt{TestNG} implements parametrized tests in a more flexible manner. However, in practice the \texttt{testng.xml} is not that widely used and the \mintinline{java}{Collections<Object[]>} \texttt{JUnit4} API is arguably cleaner than the generic-free way of \texttt{TestNG}. Finally, \texttt{JUnit4} is usually readily provided by Java IDEs and does not require additional setup. The feature parity with \texttt{TestNG} and general availability were the deciding factors in using this testing framework for lyra2-java.

Figure \ref{fig:junit4-parametrization} provides a simplified example of the way unit tests are organized in lyra2-java. The testing logic is in the \mintinline{java}{simpleTest} method: configuration parameters are constructed using the values from the \mintinline{java}{entry} instance variable, then a call to \mintinline{java}{Lyra2.phs} is made and the resulting hash is compared to the correct answer by the \mintinline{java}{assertArrayEquals} call.

The \mintinline{java}{setupClass} method is marked with the \mintinline{java}{@Parametrized.Parameters} which makes this method the provider of data. The precomputed data comes from several \texttt{YAML} data files whose names are stored in the \mintinline{java}{fnames} variable. More information about the file structure and \texttt{YAML} in general is in appendix \ref{chapter:yaml}. The \mintinline{java}{for} loop on line \(12\) sequentially opens the \texttt{YAML} data files, loads their contents and collects them in the \mintinline{java}{entries} variable. Once the \mintinline{java}{setupClass} method returns that variable, it is used by \texttt{JUnit} to initialize several instances of the class, providing each constructor with one element from the \mintinline{java}{entries} array.

This is the application of the parametrized tests approach which allows to run several hundreds of tests for different configurations and input values while writing the test code only once.

\subsection{Unit Testing with \texttt{py.test}}
\label{sec:unit-pytest}

Python is a very popular high-level general purpose scripting language created by Guido van Rossum. It has a built-in unit testing framework called \texttt{unittest} which comes together with the interpreter. It keeps a fairly compatible interface with the \texttt{xUnit} architecture described in \ref{sec:unit-junit-testng}. Other two notable unit testing frameworks in Python are \texttt{Node} and \texttt{py.test}. They both support an \texttt{xUnit} compatible layer and come as an external dependency. The features offered by these two external frameworks are superior to those provided by the built-in \texttt{unittest}.

\emph{Parametrized testing} is tricky when it comes to a scripting language like Python. Even the built-in \texttt{unittest} framework could dynamically generate test classes, as demonstrated on stackoverflow \footnote{https://stackoverflow.com/a/20870875/1269892}. However, there is no \emph{designed} way to do so. The \texttt{Nose} library allows for parametrized testing through an elegant mechanism of Python generators (the \mintinline{python}{yield} keyword) that return functions to be run as test cases. The \texttt{py.test} framework goes the more \texttt{xUnit}-inspired route of using the \mintinline{python}{@mark.parametrized} decorator.

Unfortunately, the \texttt{Nose} unit testing framework has been in maintenance mode. This framework is no longer in active development and therefore is not recommended for new projects. This is why the Python build harness for the reference Lyra2 implementation uses \texttt{py.test}.

\begin{figure}
\begin{minted}[linenos]{python}
import pytest
import subprocess

from pathlib import Path

bindir = Path(__file__).parent.parent.joinpath('bin42')
@pytest.mark.parametrize('path', list(bindir.glob("lyra2-*")))
@pytest.mark.parametrize('pwd', ['password', 'qwerty'])
@pytest.mark.parametrize('salt', ['salt', 'pepper'])
@pytest.mark.parametrize('k', [1, 2])
@pytest.mark.parametrize('t', [1, 2])
@pytest.mark.parametrize('m', [3, 4, 5, 6, 7, 8, 9, 10])
def test_sanity_0(path, pwd, salt, k, t, m):

    result = subprocess.run([path, pwd, salt, str(k), str(t), str(m)])

    assert result.returncode == 0
\end{minted}
\caption{A simplified example of Lyra2 parametrized testing with \texttt{py.test}.}
\label{fig:pytest-parametrization}
\end{figure}

Figure \ref{fig:pytest-parametrization} shows why parametrization testing is important when building different configurations of the reference Lyra2 implementation. Before running those configurations with a long list of parameters it is worth to make sure that the compiled executables actually work. However, writing a test for each individual possible configuration is not feasible. Instead, the first decorator \mintinline{python}{@pytest.mark.parametrize('path')} fetches all filenames that start with \texttt{lyra2-} (convention for Lyra2 executable names) from a specific directory. These names are later passed into a specific unit test in the \texttt{path} parameter.

This parametrized testing approach allows to write the testing logic once and reuse it for any possible Lyra2 configuration produced by the \texttt{Makefile} of the reference project.
