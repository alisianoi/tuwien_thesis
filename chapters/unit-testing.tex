\chapter{Unit Testing Framework Choice}
\label{sec:unit-testing-framework-choice}

Chapter \ref{sec:unit-testing-framework-choice} discusses the choice of particular unit testing frameworks for the \texttt{lyra2-java} project. For an introduction to unit testing terms and basic practices please consult Section \ref{sec:unit-testing-fundamentals}. In this chapter, however, Section \ref{sec:unit-boost-google} discusses how two similar projects, the \texttt{lyra2-c} project and Argon2, address testing. Section \ref{sec:unit-junit-testng} and Section \ref{sec:unit-pytest} motivate the choice of tools and techniques for the \texttt{lyra2-java} project and the Python build harness.

Although the \texttt{lyra2-c} project has automated tests, it does not use a dedicated unit testing framework \cite{github:2017:lyra}. This is a justifiable choice which has its advantages. In particular, it allows to keep the complexity of the project under control and not rely on external libraries. The build process is somewhat simplified as well.

Furthermore, when it comes to the C/C++ ecosystem, the choice of the framework is not trivial. The authors of Argon2 have also faced this choice and opted out to write their own test harness \cite{github:2017:argon2-test.c}. Section \ref{sec:unit-boost-google} will provide a plausible explanation for this decision. Section \ref{sec:unit-junit-testng} will provide motivation for the choice of the unit testing framework for the \texttt{lyra2-java} project and Section \ref{sec:unit-pytest} will describe the test suite for the Python build harness.

\begin{table}
\begin{tabular}{llll}
    Name & License & Parametrization & Parallelism \\ \hline
Boost.Test & Boost Software License & yes & needs 3\textsuperscript{rd} party runner (\texttt{cmake}) \\
JUnit & Eclipse Public License & yes & needs 3\textsuperscript{rd} party runner (\texttt{mvn}) \\
py.test & MIT & yes & needs 3\textsuperscript{rd} party plugin (\texttt{xdist})
% Google Test & MIT & parameters? & yes
\end{tabular}
\caption{Important Features of Several Unit Testing Frameworks}
\label{table:framework-features-cpp}
\end{table}

\section{Boost.Test}
\label{sec:unit-boost-google}

There is a large choice of unit testing frameworks for the C/C++ ecosystem \cite{github:2017:google-test, github:2017:catch, boost:2017:test-docs}. Given the number of available solutions, their comparison and an educated choice would be a large and daunting task for any developer. This could be one of the reasons why project authors sometimes avoid using a unit testing framework altogether. Instead, they often write a separate test file that runs a few sanity checks.

For example, Argon2 uses its own \texttt{src/test.c} test program which was introduced on 25\textsuperscript{th} of January 2016 \cite{github:2017:argon2}. The commit hash starts with \texttt{7450df88} and it is number 317 out of (current) almost 600 in the version control history. This shows that this testing was introduced at the later stages of the project when the need for it was apparent \cite{github:2017:argon2-issue-85}.

A similar story could be observed for the reference Lyra2 project \cite{github:2017:lyra}. The rest of this section will attempt to demonstrate why a rather popular unit testing library Boost.Test was not used in either of the projects. This particular library was tried because of the personal developer preference of the author of this paper. A different developer could attempt the same steps with another library, like Catch \cite{github:2017:catch} or Google Test \cite{github:2017:google-test}.

Boost itself is a large collection of different libraries for C++ \cite{boost:2017:homepage}. It was originally founded by Dawes and Abrahams but today the number of contributors is in the hundreds. The scope of the libraries spans from concurrent programming to regular expressions to linear algebra. There is a formal submission process for any library that would like to be included into the collection \cite{boost:2017:submission-process} as well as rigorous review in the mailing lists \cite{boost:2017:mailing-list}.

The Boost.Test is a unit testing framework which is part of Boost \cite{boost:2017:test-docs}. It is licensed under the Boost Software License which is a free software license, approved by the Open Source Initiative and compatible with the \gls{gpl}. The library documentation can be found online \cite{boost:2017:test-docs}.

The main features of the library are summarized in \autoref{table:framework-features-cpp}. When it comes to password hashing, support for test parametrization becomes important because of the common test scenario described in Section \ref{sec:software-compatibility}. Boost.Test provides parametrization as well as other more common features of a unit testing library. In particular, the tests can be completely decoupled from the implementation, subdivided  into test suits and automatically registered with the test runner. The registration can be accomplished with a single \texttt{\#include<...>} and a call to the \texttt{BOOST\_AUTO\_TEST\_CASE} macro, as shown in \autoref{fig:boost-auto-test-case}.

\begin{listing}
\centering
\begin{minted}{cpp}
#define BOOST_TEST_MODULE example_module_name
#include <boost/test/included/unit_test.hpp>

BOOST_AUTO_TEST_CASE(name_of_test_function) {
    BOOST_TEST(true);
}
  \end{minted}
  \caption{Automatic Unit Test Registration With Boost.Test}
  \label{fig:boost-auto-test-case}
\end{listing}

Apart from that Boost.Test has a few convenience features. First of all, it was designed to be used as a header-only library. This means that the build process of a project needs only slight modification. Secondly, Boost.Test supports parametrized tests which are called "Data-driven Test Cases" and can be found in the documentation \cite{boost:2017:test-data-driven}. First step is to declare such tests with the special \texttt{BOOST\_DATA\_TEST\_CASE} macro. This macro informs the test runner that the corresponding test case requires test data in order to function. The data must be wrapped into a dataset in order to be passed into the test case, as described in the documentation \cite{boost:2017:test-docs-dataset}.

Password hashing algorithms often require several parameters in order to run, i.e. at least a password and a salt. In the case of Boost.Test it means that datasets must be somehow combined and manipulated together. For that reason such operations like \emph{joins}, \emph{zips} and \emph{grids} are supported by the dataset wrapper \cite{boost:2017:test-docs-dataset-operations}.

In conclusion, Boost.Test requires considerable setup in order to provide parametrized testing for password hashing algorithms. Although the documentation offers an accurate description of the process, it still takes significant effort and time to configure the test harness. The \texttt{lyra2-c} project and Argon2 understandably avoid these development costs by using their own unit testing programs.

\section{JUnit and TestNG}
\label{sec:unit-junit-testng}

The Java ecosystem offers JUnit \cite{junit:2017:homepage}, which is a unit testing framework inspired by xUnit. The basic components of this framework are described below.

The main element of JUnit is a \emph{test case} which is usually a class that contains testing logic. Such a class might have special methods called \emph{fixtures} which are responsible for setting up and tearing down the context in which a test is executed. For example, this might include repeatedly creating a set of objects or opening a database connection. The methods marked with the \texttt{@Before} (respectively, \texttt{@After}) annotation are executed before (respectively, after) each individual test. The \texttt{@BeforeClass} (respectively, \texttt{@AfterClass}) annotation ensures that a method is run exactly once before (respectively, after) the test class is instantiated.

Several test cases can be collected into a single \emph{test suite}. Finally, a program called a \emph{test runner} is responsible for discovering the test cases, running them and reporting the results back to the user. The test runner can be instructed to run (or skip) specific test suits.

As well as JUnit, the Java ecosystem has a unit testing framework called TestNG \cite{testng:2017:home}. Below is a short comparison of the functionality relevant to testing password hashing algorithms.

Firstly, both frameworks offer parametrization tests but the implementation details are different. JUnit4 requires the developer to mark the class with the \texttt{@RunWith} annotation. That annotation must specify \texttt{Parameterized.class} as the test runner for the class. As well as that, the class must provide a method marked with the \texttt{@Parameters} annotation. This method should return a single \texttt{Collections<Object[]>} collection of test data \cite{junit:2017:parametrized-testing}. On the other hand, TestNG has two distinct mechanisms, one of which uses the \texttt{testng.xml} file and the other relies on a method with the \texttt{@DataProvider} annotation. In the second case the method should return either a single \texttt{Object[][]} array of test data or an \texttt{Iterator<Object[]>} iterator over such an array. With the first mechanism, the test data is stored directly in the \texttt{testng.xml} file. So, only the second approach allows to generate test data while  the tests are being run \cite{testng:2017:parametrized-testing}.

Based on the comparison above, the \texttt{lyra2-java} project was developed with JUnit4. There is nothing that would prohibit the usage of the TestNG framework for the same task. However, JUnit4 offers a single approach to writing parametrized unit tests which is flexible enough and does not make the developer choose between the different methods and different return values, as TestNG does. Finally, JUnit4 is often provided by default in modern Java development environments and does not require additional setup. Such availability has also played a major part in the choice of the unit testing framework for the \texttt{lyra2-java} project.

\begin{listing}
\small
\begin{minted}[linenos]{java}
@RunWith(Parameterized.class)
public class Lyra2Test {
    @Parameterized.Parameters
    public static Collection<Object[]> setupClass() {
        // Simplified initialization of a YAML data loader
        Yaml yaml = new Yaml();

        // A list of YAML file names with test vectors and resulting hashes
        String[] fnames = new String[] {"test-data-file_0.yml"};
        List<Object[]> entries = new ArrayList<>();

        for (String fname: fnames) {
            // Simplified loading of the YAML data from the file
            for (Object data : yaml.loadAll(reader)) {
                entries.add(new Object[]{data});
            }
        }

        return entries; // data provider has finished, returning result
    }

    private DataEntry entry;

    // Injection of test parameters
    public Lyra2Test(DataEntry entry) {
        this.entry = entry;
    }

    @Test
    public void simpleTest() {

        LyraParams params = new LyraParams(/* use entry to initialize */);

        byte[] hash = new byte[entry.klen];
        byte[] pass = entry.pass.getBytes();
        byte[] salt = entry.salt.getBytes();

        // Run the computation
        Lyra2.phs(hash, pass, salt, params);
        // Fetch the correct hash value
        byte[] correct_hash = pack.bytes(entry.hash);
        // Compare the computation result to the correct hash
        assertArrayEquals(correct_hash, hash);
    }
}
\end{minted}
\normalsize
\caption{Example of JUnit4 Parametrized Testing for the \texttt{lyra2-java} Project}
\label{fig:junit4-parametrization}
\end{listing}

\autoref{fig:junit4-parametrization} provides a simplified example of the way unit tests are organized in the \texttt{lyra2-java} project. The testing logic is in the \texttt{simpleTest} method: configuration parameters are constructed using the values from the \texttt{entry} instance variable, then a call to the \texttt{Lyra2.phs} method is made to compute the hash. The result of the computation is then compared to the correct answer by the call to the \texttt{assertArrayEquals} method.

The \texttt{setupClass} method is marked with the \texttt{@Parametrized.Parameters} annotation which makes this method the provider of data. The precomputed data comes from several \gls{yaml} data files whose names are stored in the \texttt{fnames} variable. More information about the \gls{yaml} format in general and the test data file structure in particular can be found in Section \ref{sec:configuration-and-test-file-format}. The \texttt{for} loop on line 12 sequentially opens the \gls{yaml} data files, loads their contents and collects them in the \texttt{entries} collection variable. Once the \texttt{setupClass} method returns that variable, it is used by JUnit to initialize several instances of the class, providing each constructor with one set of test data from the \texttt{entries} collection.

This is the application of the parametrized testing approach which allows to run several hundreds of tests for different configurations and input values while writing the logic for the testing code only once.

\section{Unit Testing With py.test}
\label{sec:unit-pytest}

Python is a popular high-level general purpose scripting language created by van Rossum \cite{python:2017:homepage}. It has a built-in unit testing framework called unittest \cite{python:2017:unittest-homepage} which comes together with the interpreter. It keeps a fairly compatible interface with the xUnit architecture described in Section \ref{sec:unit-junit-testng}. Other two notable unit testing frameworks in the Python ecosystem are Nose \cite{nose:2017:homepage} and py.test \cite{pytest:2017:homepage}. They both support an xUnit compatible layer and come as an external dependency.

The design of the unittest framework does not offer a dedicated mechanism to write parametrized tests. At the same time the Nose library implements parametrized testing by making use of Python generators (the \texttt{yield} keyword). They generate separate functions at runtime which are then executed as standalone test cases \cite{nose:2017:parametrized-testing}. The py.test framework, on the other hand, does not use Python generators but follows the xUnit architecture and uses the \texttt{@mark.parametrized} decorator \cite{pytest:2017:parametrized-testing}. A usage example can be found in \autoref{fig:pytest-parametrization}.

Unfortunately, the Nose unit testing framework is currently in maintenance mode. This framework is no longer in active development and therefore is not recommended for new projects. This is why the Python build harness for the \texttt{lyra2-c} project uses the py.test framework.

\begin{listing}
\begin{minted}[linenos]{python}
import pytest
import subprocess

from pathlib import Path

bindir = Path(__file__).parent.parent.joinpath('bin42')
@pytest.mark.parametrize('path', list(bindir.glob("lyra2-*")))
@pytest.mark.parametrize('pwd', ['password', 'qwerty'])
@pytest.mark.parametrize('salt', ['salt', 'pepper'])
@pytest.mark.parametrize('k', [1, 2])
@pytest.mark.parametrize('t', [1, 2])
@pytest.mark.parametrize('m', [3, 4, 5, 6, 7, 8, 9, 10])
def test_sanity_0(path, pwd, salt, k, t, m):

    result = subprocess.run([path, pwd, salt, str(k), str(t), str(m)])

    assert result.returncode == 0
\end{minted}
\caption{Simplified Example of py.test Parametrized Testing for Python Build Harness}
\label{fig:pytest-parametrization}
\end{listing}

\autoref{fig:pytest-parametrization} shows why parametrized testing is important when building different configurations of the reference Lyra2 implementation. Before running those configurations it is reasonable to make sure that the compiled executable files actually work. However, since there are many parameters that can be configured at compile time (such as the number of columns or the size of the block of the memory matrix), writing a test for each individual possible configuration is not feasible. Instead, each generated executable file is prefixed with \texttt{lyra2-} and has other compile time parameters listed in the filename, like the aforementioned number of columns or size of the block. Then the \texttt{@pytest.mark.parametrize('path')} decorator is used to fetch all the matching filenames from the build directory. Each of these names is later used as the \texttt{path} parameter to create the corresponding unit test case dynamically.

This parametrized testing approach allows to write the testing logic once and reuse it for any possible Lyra2 configuration produced by the \texttt{Makefile} of the reference project.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Configuration and Test File Format}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:configuration-and-test-file-format}

The data for parametrized tests is stored in \gls{yaml} files. Those files are produced by the \texttt{lyra2-c} project with the help of the Python build harness. Then they are consumed by the JUnit testing framework of the \texttt{lyra2-java} project. This mechanism allows to make sure that both projects compute the same hash values.

YAML stands for \emph{YAML Ain't Markup Language} and is a superset of \gls{json}, another popular data format. \gls{yaml} targets human readability and provides native support for custom datatypes: scalars, arrays, structures, etc. \cite{yaml:2017:specification}. Most programming languages provide support for reading and writing of \gls{yaml} files, including both Java and Python.

A single file in \gls{yaml} can hold several documents. This feature is leveraged by the Python build harness. In particular, each \gls{yaml} file corresponds to a single Lyra2 executable. This guarantees that exactly one Lyra2 executable is used to compute the test data hashes from that file. So, the compile time parameters are the same for the contents from that file. These parameters are stored together with all the runtime parameters and the resulting hash, see \autoref{fig:yaml-data} for reference. The \texttt{hash} field is stored as an array of strings which ensures that different \gls{yaml} libraries deduce the content type of this array correctly. The \texttt{-{}-{}-} is a delimiter between the two \gls{yaml} documents.

The \gls{yaml} format is also used to store default configuration for the Python build harness of the reference Lyra2 project. It can be found in the \texttt{harness} branch of the forked reference repository in the \texttt{harness.yml} file \cite{github:2017:lyra-copy}. The primary parameters are \texttt{build\_path} and \texttt{makefile\_path}. The first one defines the location for the compiled Lyra2 executable files and the second one points to the original \texttt{Makefile}.

The \texttt{matrix} group of parameters defines the build matrix. The \texttt{option} parameter configures the type of Lyra2 executable built by the \texttt{Makefile}, which is a generic version for the \texttt{x86\_64} architecture by default. The \texttt{threads} parameter determines the parallelism degree and is set to 1. The \texttt{columns}, \texttt{sponge}, \texttt{rounds} and \texttt{blocks} correspond directly to compile time Lyra2 parameters. Finally, the \texttt{bench} parameter determines if the test vectors should be included into the compiled executable. By default it is set to 0 which means those vectors are skipped.

The \texttt{data} group of parameters specifies the test vectors which will be used when generating hash values with \texttt{./harness.py compute}. That group includes \texttt{pass}~---~an array of passwords, \texttt{salt}~---~an array of salts, \texttt{klen}~---~an array of output lengths (i.e. the length of the hash), \texttt{tcost}~---~an array of time costs and finally \texttt{mcost}~---~an array of memory costs. The \texttt{./harness.py} script generates a grid of all of the possible value combinations and then runs every compiled executable using those values as test data. The resulting hashes are stored in the \texttt{data\_path} directory which can be configured as well. The \texttt{./harness.py} script does not overwrite old hash values, so you must remove them manually if regeneration is required.

Finally, compilation flags are also part of the configuration and their defaults can be seen in \autoref{fig:compile-flags}.

\begin{listing}
    \begin{minted}{yaml}
    blocks: 8
    columns: 16
    hash: [0f, ee, bd, 1f, '00', 2a, 5b, '87', '71', ee]
    klen: 10
    mcost: 3
    pass: password
    rounds: 1
    salt: s
    sponge: blake2b
    tcost: 1
    threads: 1
    ---
    blocks: 8
    columns: 16
    hash: [0f, 7d, e3, 3c, e3, 9e, 0c, f9, 8e, '70']
    klen: 10
    mcost: 10
    pass: password
    rounds: 1
    salt: s
    sponge: blake2b
    tcost: 1
    threads: 1
    \end{minted}
    \caption{Example of a YAML Test Data File: distinct documents are separated with \mintinline{shell}{---}.}
    \label{fig:yaml-data}
\end{listing}


\begin{listing}
    \begin{minted}{yaml}
    CFLAGS:
      - -std=c99
      - -Wall
      - -pedantic
      - -O3
      - -msse2
      - -ftree-vectorizer-verbose=1
      - -fopenmp
      - -funroll-loops
      - -march=native
      - -Ofast
      - -mprefer-avx128
      - -flto
      \end{minted}
      \caption{Compilation Flags Used by the \texttt{lyra2-c} Project}
      \label{fig:compile-flags}
  \end{listing}
