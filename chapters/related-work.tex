\chapter{Related Work}
\label{chapter:related-work}

As mentioned in Chapter \ref{sec:introduction}, this paper describes three stages during which a user's credentials must be protected. In particular, Section \ref{sec:secure-authentication} describes secure authentication, Section \ref{sec:secure-communication} covers secure communication and Section \ref{sec:secure-password-storage} discusses secure password storage. Each section provides an overview of common threats as well as defensive techniques. The chapter culminates with an introduction to memory-hard password hashing algorithms.

\section{Secure Authentication}
\label{sec:secure-authentication}

This section presents a brief overview of different methods for secure authentication and some of the accompanying types of attacks. The presented taxonomy was in part borrowed from Raza et al. \cite{raza:2012:password-attacks-survey} and then expanded and filled with details from other sources.

\subsection{Authentication Approaches}
\label{sec:authentication-approaches}

Several authentication approaches have emerged over the years. Some of them are really widespread and quite old.  At the same time others are rather novel and aim to overcome known issues. Below is a list of common authentication schemes.

\emph{Conventional Passwords} are by far the most common method of user authentication \cite{bonneau:2015:passwords-and-evolution-of-auth}. The service challenges the user to produce a pair of a (public) username and a (private) password. If the provided pair is correct, the user is allowed to proceed, otherwise the request is rejected. Such an intuitive approach works reasonably well but there are a number of fundamental issues.

 According to sources from Ives, Walsh and Schneider \cite{ives:2004:domino}, a typical user daily accesses roughly 15 services that require a password. At the same time, the user remembers on average 5 unique passwords. This suggests that some of those passwords are reused for several services. Even if attempts are made to slightly tailor the password to the service, this reuse still has a devastating effect on security, as shown by Gaw and Felten in \cite{gaw:2006:password} or Shay et al. in \cite{shay2010encountering}. The most protected account becomes as secure as the least protected one and the possibility of a breach grows with the number of services that share the same (or a similar) password.

Conventional passwords have been shown to have many flaws, see e.g. Yan et al. \cite{yan:2004:password}, Braz and Robert \cite{braz:2006:security} or Jobusch and Oldehoeft \cite{jobusch:1989:survey-of-password-mechanisms-1, jobusch:1989:survey-of-password-mechanisms-2}. Therefore it is not surprising that a number of viable alternatives has been developed over the years.

\emph{Graphical Passwords} are one such alternative. They can generally be subdivided into two categories: recognition and recall based, as done by Suo, Zhu and Owen in \cite{suo:2005:graphical-passwords-survey}. The \emph{recognition based} graphical passwords vary widely, the main idea being that the user has to recognize correct images and perform an action. For example, clicking on previously chosen pictures or clicking inside a convex hull created by them. Pictures of human faces are sometimes suggested because they are arguably the easiest to remember but other types of imagery are used as well. The \emph{recall based} schemes, on the other hand, expect the user to reproduce some type of a drawing. These include the well-known graphical passwords on smartphones, where the user is typically required to connect points of a \(3 \times 3\) grid with straight lines without lifting the finger off the screen. More complex approaches either let the user initiate several touch gestures (thus producing an even more detailed drawing) or write something that resembles a signature.

Some applications of graphical passwords claim better resistance to shoulder surfing attacks \cite{suo:2005:graphical-passwords-survey}. Unfortunately, these schemes usually take more time on average to perform the login when compared to conventional passwords. This adversely affects their practical application.

\emph{Keystroke Dynamics} is an elegant authentication mechanism, described e.g. by Monrose and Rubin \cite{monrose:2000:keystroke-dynamics}, which tries to capture the typing pattern of a legitimate user. Several statistics about the speed and rhythm, with which the user presses the keys, are captured: the time between consecutive keys, the time between pressing and releasing the key, the time it takes to type certain words etc. A more extensive list of metrics can be found in  Tillmann, de Halleux and Xie \cite{teh:2013:survey-keystroke-biometrics}. Those statistics are then used in a machine learning algorithm which ultimately decides if the user passes authentication or gets rejected. These algorithms include fine-tuned \(k\) Nearest Neighbors, as shown i.e. by Ivannikova, David and Hämäläinen \cite{ivannikova:2017:anomaly-detection-keystroke-dynamics}, naïve Bayes, as in Ho and Kang \cite{ho:2017:onenb}, gaussian mixture models, as in Deng and Zhong \cite{yunbin:2013:gmm-keystroke}, and others.

An immediate challenge is that in this scenario authentication becomes \emph{probabilistic}. Instead of asking "what the user knows", methods based on keystroke dynamics evaluate "what the user is" by judging their ability to type. This is particularly difficult in edge cases when the user is under heavy mental stress or is somehow physically restricted. One possible solution is to perform authentication as a continuous process, as shown in Borus \cite{bours2012continuous} or Serwadda et al. \cite{serwadda2013scan}. In such a scenario the user is monitored over a prolonged period of time. Similar challenges also appear with a few other biometric authentication methods, of which keystroke dynamics is a prominent example.

\emph{Biometric Authentication} is a collective term for a technique which focuses on physical traits unique to every user, as explained by e.g. Matyas and Riha \cite{matyas:2003:toward}. These include fingerprints, facial recognition, voice recognition, iris scanning and similar. Other types of biometric data as well as certain challenges connected with false recognition and false non-recognition can be found in Jain, Ross and Prabhakar \cite{jain:2004:intro-to-biometric}. That paper also highlights the notion of \emph{negative recognition} which means that biometric authentication allows to establish if the person is who they \emph{deny} to be. This property distinguishes this class of methods from all the others described in this section.

\emph{One-time Passwords} is another elegant solution for user authentication. As defined by Rayes \cite{rayes:2005:otp}, it is a password that is used exactly once and then immediately discarded. Such a password is usually used in a multi-factor fashion, specifically \gls{2fa}. This means that the user first provides the conventional username/password pair ("what the user knows") and then follows it up with a one-time password. This second password is expected to come from a dedicated smart card, a separate keyfile or a smartphone application ("what the user has"). Of major interest are two one-time password generation algorithms: \gls{hotp}, specified by M’Raihi et al. \cite{rfc4226}, and \gls{totp}, also by M’Raihi et al. \cite{rfc6238}. They are both designed as part of the \gls{oath}. The demand for such algorithms as well as a system that uses conventional smartphones for \gls{2fa} was demonstrated for example by
Aloul, Zahidi and El-Hajj \cite{aloul:2009:two-factor-auth}.

\subsection{Conventional Password Strength}
\label{sec:password-strength}

Previous Section \ref{sec:authentication-approaches} describes several approaches to user authentication. It mentions that conventional passwords are the most widespread means of user authentication \cite{bonneau:2015:passwords-and-evolution-of-auth}. That section also shows that conventional passwords have their set of problems \cite{yan:2004:password, braz:2006:security, jobusch:1989:survey-of-password-mechanisms-1, jobusch:1989:survey-of-password-mechanisms-2}. Therefore, being able to distinguish between strong and weak passwords is important. Several methods to do so are presented below.

\emph{Information Entropy} is the usual way to assess password strength, consult e.g. Bonneau \cite{bonneau:2012:the-science-of-guessing}. It is a base-two logarithm of the number of guesses which are required to recover a password with complete certainty. Information entropy is measured in bits. For example, if the password is completely random and has \(x\) bits of information entropy, then an attacker must compute \(2^x\) password candidates to be guaranteed a successful recovery.

Human generated passwords, however, are notorious for not being truly random. Which is why computing information entropy often produces inaccurate results \cite{bonneau:2012:the-science-of-guessing}.

\emph{NIST Special Publication 800-63-2} by Burr et al. \cite{burr2013electronic} tried to address this problem and suggested a refined way of computing information entropy for passwords:

\begin{itemize}
    \item The entropy of the 1\textsuperscript{st} character is 4 bits.
    \item The entropy of characters 2 to 7 is 2 bits per character.
    \item The entropy of characters 9 to 21 is 1.5 bits per character.
    \item The entropy of characters 21 and above is 1 bit per character.
    \item If \emph{both} uppercase letters and non-alphabetic characters are used, the password gains 6 more bits of entropy.
    \item If the password is 1 to 19 letters long and passes an extensive dictionary check, then an additional 6 bits of entropy should be added.
  \end{itemize}

The last bonus is not assigned to passwords of length 20 and above because it is assumed that those are \emph{pass phrases}, i.e. passwords which consist of several dictionary words. This refined approach was still criticized for being inaccurate and was removed from the revised version of the publication by Grassi, Garcia and Fenton \cite{grassi2017digital}.

\emph{Guess Number Calculation} is a different approach to estimating password strength suggested by Kelley et al. \cite{kelley2012guess}. The idea behind this method is to first choose a particular algorithm for password cracking which is known to work well. The number of password candidates which this algorithm needs to try before recovering the target password becomes the indicator of this password's strength. Such an approach was shown to perform reasonably well but it also comes with its own challenges, as pointed out by Weir et al. \cite{weir2010testing} or Zhang, Monrose and Reiter \cite{zhang2010security}. First of all, the choice of the algorithm is arbitrary. Secondly, many state of the art password cracking algorithms require a training set to fine tune their parameters. Changing the training set essentially produces a different configuration of the algorithm. This in turn leads to a recalculation of guess numbers. So, this approach in general is mainly empirical and often volatile.

\subsection{Authentication Attacks}
\label{sec:authentication-attacks}

Previous sections, namely Section \ref{sec:authentication-approaches} and Section \ref{sec:password-strength}, describe authentication approaches and password strength estimation. Section \ref{sec:authentication-attacks} provides a summary of some of the common types of authentication attacks.

\emph{Brute Force} is the most common and least sophisticated type of a password attack, see e.g. Narayanan and Shmatikov \cite{narayanan:2005:fast}. The attacker begins by choosing a specific password domain and then launches the trial and error guessing process. For example, one could try all passwords that are up to 8 characters in length and consist of lowercase Latin letters. This domain contains \(217180147158\) unique passwords. Assuming that the attacker can try \(1000\) passwords per second, it will take them \(\approx 6.8\) years to complete the search. This might sound like a lot of time but the \(1000\) passwords per second is a conservative speed estimate. Depending on the available hardware, the hash algorithm and other factors, this speed could be a \emph{a lot} higher. Table \ref{table:hashcat-speed} shows some real world speeds for a few password hashing algorithms as measured by running a common hash computation tool on ordinary consumer hardware available to the author of this paper.

\begin{table}
    \begin{center}
        \begin{tabular}{lr}
            Hashtype & Millions of hashes per second \\
            \hline
            MD5 & 2912.4 \\
            SHA1 & 1004.6 \\
            SHA-512 & 117.6 \\
            SHA-3 (Keccak) & 104.6
        \end{tabular}
    \end{center}
    \caption{Hashcat v3.6.0 Running on a Single Nvidia GTX 950M Graphics Card}
    \label{table:hashcat-speed}
\end{table}

\emph{Dictionary Attacks} are an improvement upon brute force \cite{narayanan:2005:fast}. A dictionary is a collection of possible passwords, often ordered by popularity or likelihood of occurrence. Such dictionaries are compiled from various sources: conventional language dictionaries, website pages, previously leaked passwords and so on. Advanced dictionaries or software that uses them often include \emph{mutations}. A mutation is a modification rule applied to a dictionary entry. An example of a popular modification rule might be a substitution of the letter \(o\) with the digit \(0\) or an addition of \(42\) to the end of the password. Dictionary attacks with modification rules are the most common academic approach to password cracking \cite{zhang2010security, weir2010testing, kelley2012guess, shay2010encountering}.

Even though Dictionary Attacks are the standard go-to method for password recovery research, there are several problems with this approach \cite{bonneau:2012:the-science-of-guessing}. Rather often it is difficult or outright impossible to accurately compare the results of different studies. For example, the exact password dictionaries may not be available to every researcher because they must be purchased or have changed since the date of the study. The popular tools used for the analysis are also being constantly developed, so keeping track of the exact versions is cumbersome \cite{bonneau:2012:the-science-of-guessing}.

\emph{Markov Model Based Attacks} are an example of machine learning techniques applied to password recovery, consult e.g. Dell’Amico, Michiardi and Roudier \cite{dell:2010:password}. When constructing the next password candidate, these algorithms use the beginning of the string to determine its end. For example, if the first part of the password candidate is "deter", then the Markov model will try "determine" and "detergent" earlier than "deteraaa" or "deter000". The algorithm uses training data to fit a probability distribution over the domain of possible passwords. Once the distribution is constructed, the most probable password candidates are tried first, which in practice noticeably increases the speed of password recovery. The authors of \cite{dell:2010:password} made an interesting observation as well. Intuitively, the best training data is the set of actual passwords themsevles. However, for the password data in \cite{dell:2010:password} it was shown that the second best training data is the set of usernames. This is an intriguing observation: after all, usernames and passwords serve different purposes. The explanation provided was that both the username and the password must be later recalled by the user and that during registration the user produces both their username and password in quick succession, possibly reusing the same thinking patterns.

\emph{Shoulder Surfing} is an entirely different kind of a password attack \cite{raza:2012:password-attacks-survey, suo:2005:graphical-passwords-survey} . As the name suggests, its simplest form is a person standing next to the user who is performing authentication. However, this is by far not the only scenario. The user might be authenticating from a café, a bank, an airport or any other location with sufficiently advanced video surveillance system. If a high resolution recording of the user's authentication process is captured, then this video becomes a viable attack vector.

\emph{Phishing Attacks} are yet another formidable type of an attack, as summarized by Hong \cite{hong2012state} or Wu, Miller and Garfinkel \cite{Wu:2006:STA:1124772.1124863}. A legitimate user is usually tricked into visiting a webpage which impersonates a typical webservice, like an e-mail provider or a social network. The user is then prompted to enter their authentication details. If they do not recognize the attack in time then their credentials will be compromised. Phishing attacks usually target a large group of individuals which means they do not use personalized information. On the other hand, \emph{spear phishing attacks} is a term reserved for targeting specific individuals (journalists, politicians, high-profile executives, etc.) in a much more convincing and effective way.

\section{Secure Communication}
\label{sec:secure-communication}

Having described methods of secure authentication in Section \ref{sec:secure-authentication}, we shall discuss the two approaches to establishing a secure communication channel over an insecure connection next.

\subsection{Secure Sockets Layer and Transport Layer Security}
\label{sec:ssl-tls}

Secure Sockets Layer (SSL), as standardized by Freier, Karlton and Kocher \cite{rfc6101}, and Transport Layer Security (TLS), as formalized by Dierks and Rescorla \cite{rfc5246}, is a pair of protocols widely used to ensure secure communication. TLS first appeared in 1999 and is the successor to SSL. The TLS protocol guarantees that the connection between communicating parties is \emph{secure} (i.e. a passive eavesdropper will not obtain information) and \emph{reliable} (i.e. an active attempt to tamper with the connection will be detected). The authentication of communicating parties can occur with the help of \emph{public key cryptography}. With a bit of additional configuration \emph{forward secrecy} can be achieved as well, as shown by Adrian et al. \cite{Adrian:2015:IFS:2810103.2813707}. Forward secrecy means that if at some later point the service is compromised and its private keys are leaked, the contents of the past communication sessions will not be affected.

TLS is a complex algorithm which allows \emph{a lot} of flexibility when it comes to parameter choice. It begins with a TLS \emph{handshake}:

\begin{enumerate}
    \item The client presents the list of supported ciphers and hash functions to the server.
    \item The server chooses a cipher and hash function and informs the client.
    \item Usually, the server also provides identity information by means of a digital certificate.
    \item The client decides if they want to proceed based on identity information (or lack thereof).
    \item The client and the server generate a unique session key: if perfect forward secrecy is required, then a variation of a Diffie-Hellman key exchange takes place. Otherwise, the client generates a random number and encrypts it with the server's public key. Once passed to the server, it will be decrypted with the server's private key.
  \end{enumerate}

During the handshake different algorithms can be negotiated: \gls{rsa}, Diffie-Hellman, ephemeral Diffie-Hellman, Elliptic Curve Diffie-Hellman, ephemeral Elliptic Curve Diffie-Hellman, and a few others. The digital certificate relies on the \emph{Public Key Infrastructure (PKI)} which consists of users as well as registration and certificate authorities (RAs and CAs respectively). PKI in general receives a lot of criticism:

\begin{itemize}
    \item Obtaining a digital certificate costs both time and money.
    \item CAs are being constantly breached which enables attackers to issue fake certificates: the 2011 Comodo \cite{comodo:2017:ca-incident} and DigiNotar \cite{diginotar:2017:ca-incident} incidents, the 2013 ANSSI \cite{anssi:2017:ca-incident} and TURKTRUST \cite{turktrust:2017:ca-incident-0} incidents, the 2014 National Informatics Center of India \cite{nic:2017:ca-incident} incident, the 2015 \& 2016 loss of trust in Symantec \cite{symantec:2017:ca-incident-0, symantec:2017:ca-incident-1}, 2016 \& 2017 loss of trust in WoSign and StartCom \cite{startcom:2017:ca-incident-0, startcom:2017:ca-incident-1}.
    \item Protocol versions are known to have been deprecated due to design flaws and security vulnerabilities. SSL 2.0 was formally deprecated by Polk and Turner \cite{rfc6176}, SSL 3.0  --- by Barnes et al. \cite{rfc7568}. TLS 1.2 is the current latest version of the protocol.
    \item Libraries that implement the protocol have had major security issues. One of the most well-known examples is \emph{Heartbleed}, as described by Durumeric et al. \cite{durumeric2014matter}. It was a major vulnerability that existed between 2012 and 2014 in a widely used OpenSSL library. The bug allowed anyone access to arbitrary data as a result of a buffer overflow.
  \end{itemize}

The list of incidents and issues is by no means complete. It is enough to illustrate a point though. However, since SSL/TLS is such a ubiquitous protocol, a few design and usage problems are expected to surface. The \emph{lack of trust} in CAs remains a cornerstone issue which lead to the development of a number of alternative secure communication methods that do not rely on the current PKI. One such family of methods is described in Section \ref{sec:pake} next.

\subsection{Password Authenticated Key Exchange}
\label{sec:pake}

\emph{Password Authenticated Key Exchange (PAKE)} is a family of methods that establish secure communications over an insecure channel without the need for a PKI. Insight into a select couple of protocols is provided below. In each case the described protocols provide the following guarantees:

\begin{enumerate}
    \item Resistance to offline dictionary attacks.
    \item Resistance to online dictionary attacks: only one password attempt per session.
    \item Forward secrecy: past and current session keys remain secure if the password is compromised at some later point in time.
    \item Known session key security: past and future sessions remain secure even if the current session key is compromised.
  \end{enumerate}

\emph{Encrypted Key Exchange (EKE)}, pioneered by Bellovin and Merritt \cite{bellovin1992encrypted, bellovin1993augmented}, was the first method from the PAKE family. Its early version consists of the following steps (where Alice and Bob are the names of participating parties):

\begin{enumerate}
    \item Alice and Bob both know a (weak) conventional password \(P\).
    \item Alice begins by generating a random public/private pair of keys \(E_A\) and \(D_A\). She uses \(P\) to encrypt the public key: \(M_0 = P(E_A)\). She sends \(M_0\) to Bob.
    \item Bob uses \(P\) to decrypt the public key of Alice: \(P^{-1}(P(E_A)) = E_A\). He then generates a random secret key \(R\) and encrypts it first with the asymmetric and then with the symmetric keys: \(M_1 = P(E_A(R))\). Then he sends \(M_1\) to Alice.
    \item Alice uses both \(P\) and \(D_A\) to recover \(R\): \(D_A(P^{-1}(P(E_A(R))) = R\).
  \end{enumerate}

Once this initial exchange is over, both Alice and Bob know \(R\) and \(E_A\), which are presumed to be stronger than the initial (weak) conventional password \(P\). Better strength is presumed because both \(R\) and \(E_A\) are supposed to have been generated randomly over a large possible keyspace. If the attacker controls the communication channel, then they know \(M_0 = P(E_A)\), \(M_1 = P(E_A(R))\) and \(R(\texttt{Known message})\). To try some candidate password \(P'\), they must produce a candidate \(E_A^{'} = P^{'-1}(P(E_A))\) and then decide if there exists a key \(R'\) such that \(E'_A(R') = E_A(R)\) and \(R^{'-1}(R(\texttt{Known message}))\) recovers the known message. This is expected to be far more expensive than a plain attack on \(P\).

The straightforward implementation of EKE was shown to be insecure by Hao and Ryan \cite{hao2010j}. In addition to that it is patent-encumbered. Therefore, alternative protocols have been developed.

\emph{Password Authenticated Key Exchange by Juggling (J-PAKE)} is a more recent and advanced method from the PAKE family \cite{hao2010j}. It has a formal security proof provided by Abdalla, Benhamouda and MacKenzie \cite{abdalla2015security}. Its computation cost is similar when compared to EKE. Moreover, it is not patented and has been included into such cryptographic libraries as OpenSSL and Bouncy Castle.

\section{Secure Password Storage}
\label{sec:secure-password-storage}

Section \ref{sec:secure-authentication} and Section \ref{sec:secure-communication} discussed methods for secure authentication and communication. The rest of this work focuses on the problem of secure password storage. This issue is as important as the ones discussed above because password databases are routinely compromised. As a result, protection against offline dictionary attacks is of utmost importance.

\subsection{Cryptographic Competitions}
\label{sec:cryptocomps}

It is common practice to announce a competition in order to develop standard cryptographic primitives. For example, the symmetric block cipher Rijndael by Daemen and Rijmen \cite{daemen:2002:DRA} was chosen to become the Advanced Encryption Standard (AES) \cite{aes-fips} in a competitive selection process that lasted from 1997 to 2000. The competition was organized by the National Institute of Standards and Technology (NIST) and included 15 different designs which were narrowed down to 5 during the final phase: Rijndael \cite{daemen:2002:DRA}, Serpent by Biham, Anderson and Knudsen \cite{anderson:1988:serpent}, Twofish by Schneier et al. \cite{schneier:1998:twofish}, RC6 by Rivest et al. \cite{rivest:1998:rc6} and MARS by Burwick et al. \cite{burwick:1998:mars}. Such an open standardization process was highly praised by the cryptographic community. So, another competition was held by NIST between 2007 and 2012 in order to select the next hash function standard, \gls{sha3}. The final round included 5 designs: Blake by Aumasson et al. \cite{aumasson:2013:blake2}, Grøstl by Gauravaram et al. \cite{praveen:2011:groestl}, JH by Wu \cite{wu:2011:jh}, Skein by Ferguson et al. \cite{ferguson:2009:skein} and Keccak by Bertoni et al. \cite{cryptoeprint:2015:keccak}, the last of which ultimately became the winner of the competition.

Choosing cryptographic primitives through an open competitive process is an effective approach. Therefore, when the need for an updated, memory-hard \gls{phs} became apparent, a Password Hashing Competition was held. This time, however, it was not organized by NIST but directly by the cryptographic community. In particular, the \url{password-hashing.net} (visited on 10/28/2017) website lists two well-known cryptographers Khovratovich and Aumasson as direct contacts. The Password Hashing Competition concluded in 2015, with Argon2 \cite{biryukov:2015:argon2} selected as its winner and Catena \cite{forler:2013:catena}, Lyra2 \cite{andrade:2016:lyra2}, Makwa \cite{pornin:2015:makwa} and yescrypt \cite{peslyak:2015:yescrypt} receiving special recognition.

\subsection{Password Hashing Fundamentals}
\label{sec:fundamentals}

\emph{Password hashing} is the process of transforming a password into a hash value. Given the resulting hash value, it should be computationally infeasible to restore the original password.

Password hashing is common practice when user authentication is required. The user provides a password which is then hashed using some password hashing algorithm. The resulting hash is then compared against a hash value which is stored in the database and is known to be correct. If both hashes match, the user is authenticated.

When a password database is leaked, the password hashing process is what prevents the attackers from gaining the original plaintext passwords. One of the early \glsentrylongpl{phs}, which is widely used today, is PBKDF2, described in section 5.2 of the \gls{rfc} specification by Moriarty, Kaliski and Rusch \cite{moriarty:2017:pkcs}. The fundamental idea behind it (which is shared among a few other early \glsentrylongpl{phs}) is to apply a \emph{pseudo-random function} a number of times, treating the number of repetitions as a parameter for computational cost.

One of the early kinds of attacks against password hashes were \emph{rainbow tables}, as discussed by Avoine, Junod and Oechslin \cite{Avoine:2008:CIT:1380564.1380565}. They are a classic example of a \emph{Time-memory Tradeoff Attack} (TMTO) \cite{Avoine:2008:CIT:1380564.1380565}. Given a dictionary of possible passwords (up to a certain length and using a particular set of symbols) and a hashing algorithm, an attacker precomputes hash values well in advance. When a password database of the defender is leaked, the hash values are compared to those from the rainbow table. If two hashes match, the recovery of the original password becomes a matter of a simple lookup.

The currently well-known defense against the rainbow table attack is to use a sufficiently large \emph{salt} when computing the password. This salt is a randomly generated value which is unique for every password and is openly stored alongside it in the defender's database. The salt ensures that the attacker will have to perform the computation after the leak (possibly giving a chance to the defender to change their password). Another nice property is that the same password used by several users will be very likely to produce distinct hash values.

These days an attacker can often compute a large number of hashes in parallel, as shown by Murakami, Kasahara and Saito \cite{5665047}. If so, the increased individual computational time cost of one hash does not prevent the attacker from trying many candidates at once. The throughput of the attack (average attempted passwords per second) remains high. This type of attacks is addressed by memory-hard \glsentrylongpl{phs}. The main idea is that parallel systems (such as general-purpose \glspl{gpu} or specialized \glspl{asic} and \glspl{fpga}) usually have significantly less memory per one processing unit than \glspl{cpu} of a personal computer. Therefore, if a password hashing process offers a memory cost parameter, an attacker will need (much) more memory per each specialized processing unit in order to have the same throughput. This is supposed to make the attack much more expensive and slow it down considerably.

Arguably the first \gls{phs} to implement this idea was scrypt by Percival \cite{percival:2016:scrypt}, which was recently published as \mbox{\gls{rfc} 7914} by Percival and Josefsson \cite{rfc7914}. However, this scheme is sometimes criticized for being overly complicated and not offering a decoupled way to control time and memory costs. In other words, there is a single parameter that controls both those values.

In order to create new designs which would address the need for a new memory-hard password hashing function, the Password Hashing Competition was announced in 2013 and concluded in 2015 \cite{wetzels:2016:phc}. Below follows a quick summary of its winner (Argon2 \cite{biryukov:2015:argon2}) and all of the finalists except Lyra2 \cite{andrade:2016:lyra2}. The latter will be described in more detail later in chapter \ref{chapter:lyra2}.

\subsection{Main Features of Catena}
\label{sec:catena}

Catena \cite{forler:2013:catena} is a password-scrambling framework based on Bit Reversal Graphs. One of its prominent features is \emph{client-independent updates}. It allows a hash value to be updated with larger time or memory cost values without the need to wait for a user to login. Catena also offers a \emph{server-relief} feature which allows to offload most of the hash computation to the client machine rather than the server, hence increasing the number of possible concurrent logins.

The Catena-Butterfly(-Full) and Catena-Dragonfly(-Full) are the most notable configurations of Catena. The former one is recommended when the memory-hard property is required. The -Full versions utilize the complete set of rounds of the underlying hash function Blake2b \cite{aumasson:2013:blake2}.

An interesting related project is Catena-Axungia \cite{github:2017:catena-axungia}. It allows its user to specify the desired time and memory requirements in conventional (for a human) seconds and kilobytes. After that the program returns recommended parameter values for the current machine. These values will on average make Catena-Butterfly or Catena-Dragonfly run for the set amount of time and consume the set amount of memory.

Finally, the Catena-Variants \cite{github:2017:catena-variants} project is a modular C++ implementation of Catena. It is reported to be somewhat slower and consume more memory while at the same time providing more flexibility. There is an API that allows the developer to mix and match internal parts of the algorithm.

\subsection{Main Features of Makwa}
\label{sec:makwa}

The Makwa PHS \cite{pornin:2015:makwa} at its core relies on Blum integers. A Blum integer is a natural integer \(n\) which can be represented as a product \(pq\) (where \(p\) and \(q\) are prime numbers) with an additional property:

\begin{IEEEeqnarray}{rCl}
    p &=& 3 \texttt{ mod } 4 \\
    q &=& 3 \texttt{ mod } 4
\end{IEEEeqnarray}

The core idea of Makwa is to square the (derivative hash of) the password (together with the salt and other parameters) many times modulo a Blum integer. This squaring is primarily computation intensive and does not require a significant amount of memory. The \(p\) and \(q\) integers should be kept secret, if they are known then the computation can be accelerated considerably.

One of the distinguishing features of Makwa highlighted on its website is \emph{delegation} \cite{makwa:2017:homepage}. The author points out that the complexity of password hashing is essentially an arms race between defenders and attackers. Delegation allows to use untrusted systems to perform part of the computation of the hash value with the Makwa algorithm. The exact protocol can be found in Section 4 of the specification \cite{pornin:2015:makwa-spec}.

\subsection{Main Features of yescrypt}
\label{sec:yescrypt}

The \emph{yescrypt} PHS \cite{peslyak:2015:yescrypt} improves upon its predecessor, \emph{scrypt} \cite{percival:2016:scrypt}. However, yescrypt author Alexander Peslyak makes it clear that the author of scrypt is a different person, Colin Percival. The yescrypt PHS deals with some minor inconsistencies discovered in the specification of its predecessor.

The yescrypt PHS also introduces a novel configuration with a read-only memory (ROM) table. In that configuration random lookups are performed so as to ensure that this table remains in memory. Finally, the \texttt{YESCRYPT\_RW} flag enables these lookups as well as a number of optimized instructions.

\subsection{Main Features of Argon2}
\label{sec:argon2}

\emph{Argon2} \cite{biryukov:2015:argon2} has two distinct configurations: \emph{Argon2i} and \emph{Argon2d}. The former revisits the blocks of the in-memory matrix in the data-\emph{independent} fashion while the latter does so in a data-\emph{dependent} manner. This means that Argon2i is better suited for scenarios when \emph{side-channel attacks} are a viable concern, such as during password hashing or key derivation. A side-channel attack is described by Caddy \cite{caddy:2005:side-channel} as a type of attack which exploits information relevant to the implementation of a cryptographic algorithm as well as its mathematical properties. At the same time Argon2d is more resistant to \emph{time-memory tradeoffs} which makes it more suitable for digital cryptocurrencies or other cases where proof of work is important.

Argon2 accepts the following set of parameters: password, salt, degree of parallelism, length of the produced hash (called a \emph{tag} in \cite{biryukov:2015:argon2}), memory cost, time cost, version number (for compatibility reasons, currently at \texttt{0x13}), secret value, associated data and type of configuration to use (Argon2i or Argon2d).

The more interesting parameters are the degree of parallelism as well as the secret value together with associated data. The last of these three adds more flexibility to the scheme. The secret value parameter enables \emph{keyed hashing} \cite{223865} and improves security in case of a database leak. Keyed hashing is similar to salt but the key is the same for the entire database and is stored in Random Access Memory. This does not introduce theoretical security but instead complicates the technical job for an actual attacker. Finally, the degree of parallelism directly corresponds to the number of rows of the in-memory matrix.

Argon2 is arguably the most widespread and popular algorithm among all of the finalists. Consequently, the \texttt{README.md} file in the GitHub repository \cite{github:2017:argon2} provides a long list of bindings for various languages. Finally, notable companies like the Django Software Foundation and Yandex are actively using Argon2 in production \cite{django:2017:argon2, github:2017:argonische}.

\section{Software Compatibility}
\label{sec:software-compatibility}

This paper deals with porting an algorithm from one programming language to another. Therefore the question of compatibility needs to be addressed.

One possible approach to ensure that the two programs work the same way is to utilize formal specification and verification methods, as shown by van Lamsweerde in \cite{lamsweerde:2000:formal-specification} and Müller and Poetzsch-Heffter in \cite{mueller:1994:formal-specification}. In particular, this requires building (or reusing) some kind of a theoretical model and performing rigorous analysis of the source code. In case the source code ever changes, the analysis will have to be repeated as well. This is a manual and labor-intensive process which the author of this paper would like to avoid.

Instead, the \texttt{lyra2-java} project relies on a more practical verification approach, namely unit testing. Of course, unit testing is not a strict and formal compatibility and correctness guarantee. Nonetheless it has been shown useful for building confidence in the written code by Williams and Kudrjavets \cite{williams:2010:unit-tests-rock}. Unit testing comes with its own classical questions: which tools and frameworks to use, as discussed by Daka and Fraser \cite{daka:2014:unit-testing-tools}, and how to know that enough effort has been spent on writing tests, as demonstrated by Elberzhager et al. \cite{elberzhager:2012:reducing-effort}

The upcoming Section \ref{sec:unit-testing-fundamentals} deals with the terms, definitions and best practices. Chapter \ref{sec:unit-testing-framework-choice} is devoted to the choice of particular unit testing tools for lyra2-java and the Python build harness.

\subsection{Unit Testing Fundamentals}
\label{sec:unit-testing-fundamentals}

\emph{Unit testing} is the process of verifying that the program produces expected results when given specific inputs, consult e.g. Cheon and Leavens \cite{cheon2002simple}. \emph{Unit under test} is a common term that refers to the specific part of the code that is being tested by a specific \emph{test case}. Unit testing is often powered by a \emph{unit testing framework} which is a set of tools dedicated to simplify both writing and running unit tests. \emph{Test Driven Development} is a software development practice which expects a piece of functionality to be produced together with the accompanying tests, as described e.g. by Astels in \cite{Astels:2003:TDD:864016}.

One notable property of a typical unit test is that it deals with the smallest logical piece of code possible: one particular function or a single method of a class. Consequently, distinct unit tests are usually independent of each other and can be run in any order or in parallel. Parallelized unit test execution is therefore a desirable feature of a unit testing framework.

However, there are also cases when the complexity of a unit test is higher. For example, when a class heavily relies on data coming from a database, a unit test needs to \emph{mock} the database connection and the data. \emph{Mocking} is the process of simulating a real object with a simplified version of it.

It is often the case that many unit tests share the same logic. Specifically in the context of testing hash functions, this logic could be summarized with the following steps:

\begin{enumerate}
    \item Select a configuration of the hash function.
    \item Provide input data: a password, a salt, etc.
    \item Compute the hash value and compare it to the correct one.
   \end{enumerate}

Writing a unit test for each combination of the hash function configuration and each set of input parameters is a daunting task. \emph{Parametrized unit testing} allows to generate a template for a large number of unit tests and avoid the extra labor, as shown by Tillmann, de Halleux and Xie \cite{tillmann:2010:parametrized-unit-tests-rock}. Therefore, unit testing frameworks that support this particular feature are of special interest in this work.

\emph{Code coverage} indicates how well a program is tested. It is low when only a small portion of the program is tested, and high otherwise. There are many ways to measure code coverage, some of which are described in \cite{elberzhager:2012:reducing-effort}. In practice the most common code coverage metric is the number of lines of code (LOCs) covered by the test suite as a percentage of the total number of LOCs.

Other types of coverage include: \emph{function} and \emph{statement} coverage (i.e. the portion of functions or statements that has been executed), \emph{branch} and \emph{condition} coverage (i.e. the portion of conditions/branches executed in relation to the total number of \emph{all possible} combinations) and many others \cite{Astels:2003:TDD:864016}. For the sake of simplicity, the projects developed in this work will not be using these metrics.

\emph{Continuous integration} (CI) is the practice of running unit tests (and measuring code coverage changes) with every codebase update. This approach helps identify problems early and fix them quicker \cite{williams:2010:unit-tests-rock}. Continuous integration often occurs transparently on a separate set of dedicated machines and its status is visible to the developers at all times. Its popularity and utility is indisputable, with such companies like TravisCI \cite{travis:2017:homepage}, AppVeyor \cite{appveyor:2017:homepage} and CircleCI \cite{circleci:2017:homepage} providing both commercial and free (for open source projects) continuous integration as a service.
