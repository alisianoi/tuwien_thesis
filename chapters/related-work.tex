\chapter{Related Work}
\label{chapter:related-work}

\section{Secure Authentication}
\label{sec:secure-authentication}

This section will present a brief overview of different methods for secure authentication as well as a short summary of some types of attacks. The presented taxonomy was in part borrowed from \cite{raza:2012:password-attacks-survey} and then expanded and filled with details from other sources.

\subsection{Authentication Approaches}
\label{subsec:authentication-approaches}

Several authentication approaches have emerged over the years. Some of them are much more widespread and old while others are rather novel and aim to solve some known issues. Below is a list of rather common and interesting authentication schemes.

\emph{Conventional Passwords} are by far the most widespread method of user authentication. The service challenges the user to produce a pair of a (public) username and a (private) password. If the provided pair is correct, the user is allowed to proceed, otherwise the request is rejected. Such an intuitive approach works reasonably well but still comes with a number of challenges.

 According to sources from \cite{ives:2004:domino}, a typical user daily accesses roughly 15 services that require a password. At the same time, they actually remember at most 5 unique passwords. The pigeonhole principle suggests that some of those passwords are reused for different services. Even if attempts are made to slightly tailor the password to the service, this reuse still has a devastating effect on security \cite{gaw2006password, shay2010encountering}. The most protected account becomes as secure as the least protected one and the possibility of a breach grows with the number of services that share the same (or a rather similar) password.

Advanced password guessing techniques provide interesting insight as well. The authors of \cite{dell:2010:password} use a Markov model to prioritize the order in which an attacker guesses the user's password. The algorithm uses training data to fit a probability distribution over the domain of possible passwords. Once the distribution is constructed, the most probable password candidates are tried first, which in practice noticeably increases the speed of password recovery. Intuitively, the best training data is the set of actual passwords themsevles. However, for the password data in \cite{dell:2010:password} it was shown that the second best training data is the set of usernames. This is an intriguing observation: after all, usernames and passwords serve different purposes. The explanation provided was that a) both the username and the password must be later recalled by the user and b) upon registration, the user produces both their username and password in quick succession, possibly reusing the same thinking patterns.

Despite their many flaws, conventional passwords remain the backbone of user authentication. At the same time a number of viable alternatives have been developed over the years.

\emph{Graphical Passwords} are one such alternative. They can generally be subdivided into two categories: recognition- and recall based \cite{suo:2005:graphical-passwords-survey}. The \emph{recognition based} graphical passwords vary widely, the main idea being that the user has to recognize correct images and perform an action. For example, clicking on previously chosen pictures or clicking inside a convex hull created by them. Pictures of human faces are sometimes suggested because they are arguably the easiest to remember but other types of imagery are used as well. The \emph{recall based} schemes, on the other hand, expect the user to reproduce some type of a drawing. These include the well-known graphical password on smartphones, where the user is typically required to connect points of a \(3 \times 3\) grid with straight lines without removing the finger. More complex approaches either let the user initiate several touch gestures thus producing an even more detailed drawing or write something that resembles a signature.

Some applications of graphical passwords claim better resistance to shoulder surfing attacks \cite{suo:2005:graphical-passwords-survey}. Unfortunately, these schemes usually take more time on average to perform the login when compared to conventional passwords. This adversely affects their practical application.

\emph{Keystroke Dynamics} is an elegant authentication mechanism which tries to capture the typing pattern of a legitimate user. Several statistics about the speed and rhythm with which the user presses the keys are captured: a) the time between consecutive keys b) the time between pressing and releasing the key c) the time it takes to type certain words etc. A more extensive list of examples can be found in \cite{teh:2013:survey-keystroke-biometrics}. Those statistics are then used in a machine learning algorithm which ultimately decides if the user passes authentication or gets rejected. These algorithms include fine-tuned \(k\) Nearest Neighbors \cite{ivannikova:2017:anomaly-detection-keystroke-dynamics}, na√Øve Bayes \cite{ho:2017:onenb}, gaussian mixture models \cite{yunbin:2013:gmm-keystroke} and others.

An immediate challenge is that in this scenario authentication becomes \emph{probabilistic}. Instead of asking "what the user knows", methods based on keystroke dynamics evaluate "what the user is" by judging their ability to type. This becomes particularly difficult in edge cases when the user is under heavy mental stress or is somehow physically restricted. One possible solution is to perform authentication as a continuous process \cite{bours2012continuous, serwadda2013scan} and monitor the user over a prolonged period of time. This challenge also appears with a few other biometric authentication methods, of which keystroke dynamics is a prominent example.

\emph{Biometric Authentication} is a collective term for a technique which focuses on physical traits unique to every user. These include fingerprints, facial recognition, voice recognition, iris scanning and similar. Other types of biometric data as well as certain challenges connected with false recognition as well as false non-recognition can be found in \cite{jain:2004:intro-to-biometric}. That paper also highlights the notion of \emph{negative recognition} which means that biometric authentication allows to establish if the person is who they \emph{deny} to be. This property distinguishes this class of methods from all the others described in this section.

\emph{One-time Passwords} is another elegant solution for user authentication. It is usually used in a multi-factor fashion, specifically Two-Factor Authentication (2FA). This means that the user first provides the conventional username/password pair ("what the user knows") and then follows it up with a one time password. This second password is expected to come from a dedicated smart card, a separate keyfile or a smartphone application ("what the user has"). Of major interest are two one time password generation algorithms: HMAC-based One-time Password Algorithm (HOTP \cite{rfc4226}) and Time-based One-time Password Algorithm (TOTP \cite{rfc6238}). They are both developed as part of the Initiative for Open Authentication (OATH). They are implemented by several mobile applications for various operating systems and are also supported in the backend by such companies as Google, Yandex, GitHub, GitLab and many others \cite{wiki:2017:hotp-totp-companies}.

\subsection{Conventional Password Strength}
\label{subsec:password-strength}

Conventional passwords are the most widespread means of user authentication, despite their many flaws and a number of alternative authentication approaches outlined in \ref{subsec:authentication-approaches}. Therefore, being able to distinguish strong and weak passwords is an important requirement. Several methods to do so are presented below.

\emph{Information Entropy} is the usual way to assess password strength. It is a base-two logarithm of the number of guesses which are required to recover a password with complete certainty. Information entropy is measured in bits. For example, if the password is completely random and has \(x\) bits of information entropy, then an attacker must compute \(2^x\) password candidates to be guaranteed a successful recovery.

Human generated passwords, however, are notorious for begin not truly random. Which is why computing information entropy often produces overly optimistic results.

\emph{NIST Special Publication 800-63-2} \cite{burr2013electronic} tried to address this problem and suggested a refined way of computing information entropy:

\begin{itemize}
    \item The entropy of the 1\textsuperscript{st} character is 4 bits.
    \item The entropy of characters 2 to 7 is 2 bits per character.
    \item The entropy of characters 9 to 21 is 1.5 bits per character.
    \item The entropy of characters 21 and above is 1 bit per character.
    \item If \emph{both} uppercase letters and non-alphabetic characters are used, the password gains 6 more bits of entropy.
    \item If the password is 1 to 19 letters long and passes an extensive dictionary check, then an additional 6 bits of entropy should be added.
  \end{itemize}

The last bonus is not assigned to passwords of length 20 and above because it is assumed that those are \emph{pass phrases}, i.e. passwords which consist of several dictionary words. This refined approach was still criticized for being inaccurate and was removed from the revised version of the publication \cite{grassi2017digital}.

\emph{Guess Number Calculation} is a different approach to estimating password strength \cite{kelley2012guess}. The idea behind this method is to first choose a particular algorithm for password cracking which is known to work well. The number of password candidates which this algorithm needs to try before recovering the target password becomes the indicator of this password's strength. Such an approach was shown to perform reasonably well but it also comes with its own challenges \cite{weir2010testing, zhang2010security}. First of all, the choice of the algorithm is arbitrary. Secondly, many state of the art password cracking algorithms require a training set to fine tune their parameters. Changing the training set essentially produces a different configuration of algorithm parameters. This in turn leads to a recalculation of guess numbers. So, this approach in general is mainly empirical and often volatile.

\subsection{Authentication Attacks}
\label{subsec:authentication-attacks}

Having described authentication approaches in section \ref{subsec:authentication-approaches} and password strength in section \ref{subsec:password-strength}, it is only fair to consider the common types of authentication attacks next. This section will provide an overview of most well-known techniques.

\emph{Brute Force} is the most common and least sophisticated type of a password attack. The attacker basically chooses a specific password domain and begins the trial and error guessing process. For example, one could try all passwords that are up to 8 characters in length and consist of lowercase latin letters. This domain contains \(217180147158\) unique passwords. Assuming that the attacker can try \(1000\) passwords per second, it will take them \(\approx 6.8\) years to complete the search. This might sound like a lot of time but the \(1000\) passwords per second is a conservative speed estimate. Depending on the available hardware, the hash algorithm and other factors, this speed could be a \emph{a lot} higher. Real world speeds for some hashing algorithms can be found in table \ref{table:hashcat-speed}

\begin{table}
    \begin{center}
        \begin{tabular}{lr}
            Hashtype & Millions of hashes per second \\
            \hline
            MD5 & 2912.4 \\
            SHA1 & 1004.6 \\
            SHA-512 & 117.6 \\
            SHA-3 (Keccak) & 104.6
          \end{tabular}
      \end{center}
      \caption{Hashcat v3.6.0 running on a single Nvidia GTX 950M graphics card.}
      \label{table:hashcat-speed}
  \end{table}

\emph{Dictionary Attacks} are an improvement upon brute force. A dictionary is a collection of possible passwords, often ordered by popularity or likelihood of occurance. Such dictionaries are compiled from various sources: conventional language dictionaries, website pages, previously leaked passwords and so on. Advanced dictionaries or software that uses them often includes \emph{mutations}. A mutation is a modification rule applied to a dictionary entry. An example of a popular modification rule might be a substitution of the letter \(o\) with the digit \(0\) or an addition of \(42\) to the end of the password. Dictionary attacks with modification rules are the most common academic approach to password cracking \cite{zhang2010security, weir2010testing, kelley2012guess, shay2010encountering}.

\emph{Shoulder Surfing} \cite{raza:2012:password-attacks-survey, suo:2005:graphical-passwords-survey} is an entirely different kind of an attack. As the name suggests, its simplest form is a person standing next to the user who is performing authentication. However, this is by far not the only scenario. The user might be authenticating from a caf√©, a bank, an airport or any other location with sufficiently advanced video surveillance system. If a high resolution recording of the user's authentication process is captured, then this video becomes a viable attack vector.

\emph{Phishing Attacks} \cite{hong2012state, Wu:2006:STA:1124772.1124863} are yet another formidable type of an attack. A legitimate user is usually tricked into visiting a webpage which impersonates a typical webservice, like an e-mail provider or a social network. The user is then prompted to enter their authentication details. If they do not recognize the attack in time then their credentials will get compromised. Phishing attacks usually target a large group of individuals which means they cannot use personalized information. On the other hand, \emph{spear phishing attacks} is a term reserved for targeting specific individuals (journalists, politicians, high-profile executives, etc.) in a much more convincing and effective way.

\section{Secure Communication}
\label{sec:secure-communication}

This section will present two approaches to establishing a secure communication channel over an insecure connection. It is a logical continuation of section \ref{sec:secure-authentication} which discusses methods of secure authentication.

\subsection{Secure Sockets Layer and Transport Layer Security}
\label{subsec:ssl-tls}

Secure Sockets Layer (SSL) and Transport Layer Security (TLS) is a pair of protocols widely used to ensure secure communication. TLS first appeared in 1999 and is the successor to SSL. The TLS protocol ensures that the connection between communicating parties is \emph{secure} (i.e. a passive eavesdropper will not obtain information) and \emph{reliable} (i.e. an active attempt to tamper with an encrypted message will be detected). The authentication of communicating parties can occur with the help of \emph{public key cryptography}. With a bit of additional configuration \emph{perfect forward secrecy} can be achieved as well.

TLS is a complex algorithm which allows \emph{a lot} of flexibility when it comes to parameter choice. It begins with a TLS \emph{handshake}:

\begin{enumerate}
    \item The client presents the list of supported ciphers and hash functions to the server.
    \item The server chooses a cipher and hash function and informs the client.
    \item Usually, the server also provides identity information by means of a digital certificate.
    \item The client decides if they want to proceed based on identity information (or lack thereof).
    \item The client and the server generate a unique session key: if perfect forward secrecy is required, then a variation of a Diffie-Hellman key exchange takes place. Otherwise, the client generates a random number and encrypts it with the server's public key. Once passed to the server, it will be decrypted with the server's private key.
  \end{enumerate}

During the handshake different algorithms can be negotiated: RSA, Diffie-Hellman, ephemeral Diffie-Hellman, Elliptic Curve Diffie-Hellman, ephemeral Elliptic Curve Diffie-Hellman, and a few others. The digital certificate relies on the \emph{Public Key Infrastructure (PKI)} which consists of users as well as registration and certificate authorities (RA and CA respectively). PKI in general receives a lot of criticism:

\begin{itemize}
    \item Obtaining a digital certificate costs both time and money.
    \item CAs are being constantly breached which enables attackers to issue fake certificates: the 2011 Comodo \cite{comodo:2017:ca-incident} and DigiNotar \cite{diginotar:2017:ca-incident} incidents, the 2013 ANSSI \cite{anssi:2017:ca-incident} and TURKTRUST \cite{turktrust:2017:ca-incident-0} incidents, the 2014 National Informatics Center of India \cite{nic:2017:ca-incident}, the 2015 \& 2016 loss of trust in Symantec \cite{symantec:2017:ca-incident-0, symantec:2017:ca-incident-1}, 2016 \& 2017 loss of trust in WoSign and StartCom \cite{startcom:2017:ca-incident-0, startcom:2017:ca-incident-1}.
    \item Protocol versions are known to have been deprecated due to design flaws and security vulnerabilities. SSL 2.0 was deprecated by \cite{rfc6176}, SSL 3.0  --- by \cite{rfc7568}. TLS 1.2 is the current latest version of the protocol.
    \item Libraries that implement the protocol have had major security issues. One of the most well-known examples is \emph{Heartbleed} \cite{durumeric2014matter}. It was a major vulnerability that existed between 2012 and 2014 in a widely used OpenSSL library which allowed to access arbitrary data as a result of a buffer overflow.
  \end{itemize}

In my opinion, the provided criticism is substatial. However, since SSL/TLS is such a widespread protocol, it should be expected that many users and usage scenarious will result in at least some uncovered problems. The \emph{lack of trust} in CAs remains a cornerstone issue which lead to a rise of alternative secure communication methods that do not rely on the current PKI.

\subsection{Password Authenticated Key Exchange}
\label{subsec:pake}

\emph{Password Authenticated Key Exchange (PAKE)} is a family of methods that establish secure communications over an insecure channel without the need for a PKI. Insight into a select couple of protocols is provided below. In each case the described protocols provide the following security guarantees:

\begin{enumerate}
    \item Resistance to offline dictionary attacks.
    \item Resistance to online dictionary attacks: only one password attempt per session.
    \item Forward secrecy: session key remains secure if the password is compromised.
    \item Known session key security: past and future sessions remain secure even if the current session key is compromised.
  \end{enumerate}

\emph{Encrypted Key Exchange (EKE)} was the first method from the PAKE family \cite{bellovin1992encrypted, bellovin1993augmented}. Its early version consists of the following steps:

\begin{enumerate}
    \item Alice and Bob share a (weak) conventional password \(P\).
    \item Alice begins by generating a random public/private pair of keys \(E_A\) and \(D_A\). She uses \(P\) to encrypt the public key: \(M_0 = P(E_A)\). She sends \(M_0\) to Bob.
    \item Bob uses \(P\) to decrypt the public key of Alice: \(P^{-1}(P(E_A)) = E_A\). He then generates a random secret key \(R\) and encrypts it first with the asymmetric and then with the symmetric keys: \(M_1 = P(E_A(R))\). Then he sends \(M_1\) to Alice.
    \item Alice uses both \(P\) and \(D_A\) to recover \(R\): \(D_A(P^{-1}(P(E_A(R))) = R\).
  \end{enumerate}

Once this initial exchange is over, both Alice and Bob know \(R\) and \(E_A\), which are presumed to be stronger than the initial shared secret \(P\) because they are supposed to have been generated randomly over a large possible keyspace. If the attacker controls the communication channel, then they know \(M_0 = P(E_A)\), \(M_1 = P(E_A(R))\) and \(R(\text{Known message})\). To try some candidate password \(P'\), they must produce a candidate \(E_A^{'} = P^{'-1}(P(E_A))\) and then decide if there exists a key \(R'\) such that \(E'_A(R') = E_A(R)\) and \(R^{'-1}(R(\text{Known message}))\) recovers the known message. This is expected to be far more expensive than a plain attack on \(P\).

The straightforward implementation of EKE was shown to be insecure \cite{hao2010j}. In addition to that it is patent-encumbered. Therefore, alternative protocols have been developed.

\emph{Password Authenticated Key Exchange by Juggling (J-PAKE)} is a more recent and advanced method from the PAKE family \cite{hao2010j}. It has a formal security proof \cite{abdalla2015security}. Its computation cost is similar when compared to EKE. Moreover, It is not patented and has been included into such cryptographic libraries as OpenSSL and Bouncy Castle.

\section{Secure Password Storage}
\label{sec:secure-password-storage}

Sections \ref{sec:secure-authentication} and \ref{sec:secure-communication} discussed methods of secure authentication, communication and possible attack scenarios. The rest of this work focuses on the problem of secure password storage. This issue is as important as the ones discussed in previous sections because password databases are routinely compromised. As a result, protection against offline dictionary attacks is of utmost importance.

\subsection{Cryptographic Competitions}
\label{sec:cryptocomps}

It is common practice to announce a competition in order to develop standard cryptographic primitives. For example, the symmetric block cipher Rijndael \cite{daemen:2002:DRA} was chosen to become the Advanced Encryption Standard (AES) \cite{aes-fips} in a competitive selection process that lasted from 1997 to 2000. The competition was organized by the National Institute of Standards and Technology (NIST) and included 15 different designs which were narrowed down to 5 during the final phase: Rijndael \cite{daemen:2002:DRA}, Serpent \cite{anderson:1988:serpent}, Twofish \cite{schneier:1998:twofish}, RC6 \cite{rivest:1998:rc6} and MARS \cite{burwick:1998:mars}. Such an open standartization process was highly praised by the cryptographic community. So, another competition was held by NIST between 2007 and 2012 in order to select the next hash function standard, SHA-3. The final round included 5 designs: Blake \cite{aumasson:2013:blake2}, Gr√∏stl \cite{praveen:2011:groestl}, JH \cite{wu:2011:jh}, Skein \cite{ferguson:2009:skein} and Keccak \cite{cryptoeprint:2015:keccak}, the last of which ultimately became the winner of the competition.

Choosing cryptographic primitives through an open competitive process is an effective approach. Therefore, when the need for an updated, memory-hard password hashing scheme became apparent, a Password Hashing Competition was held. This time, however, it was not organized by NIST but directly by the cryptographic community. In particular, the \url{password-hashing.net} website lists two well-known cryptographers Dmitry Khovratovich and Jean-Philippe Aumasson as direct contacts. The Password Hashing Competition concluded in 2015, with Argon2 \cite{biryukov:2015:argon2} selected as its winner and Catena \cite{forler:2013:catena}, Lyra2 \cite{andrade:2016:lyra2}, Makwa \cite{pornin:2015:makwa} and yescrypt \cite{peslyak:2015:yescrypt} receiving special recognition.

\subsection{Password Hashing Fundamentals}
\label{sec:fundamentals}

\emph{Password hashing} is the process of transforming a password into a hash value. Given the resulting hash value it should be computationally infeasible to restore the original password.

Password hashing is common practice when user authentication is required. The user provides a password which is then hashed using some password hashing algorithm. The resulting hash is then compared against a hash value which is stored in the database and is known to be correct. If both hashes match, the user is authenticated.

When a password database is leaked, the password hashing process is what prevents the attackers from gaining the original plaintext passwords. One of the early password hashing schemes, which is widely used today, is PBKDF2, described in section 5.2 of \cite{moriarty:2017:pkcs}. The fundamental idea behind it (which is shared among a few other early password hashing schemes) is to apply a \emph{pseudo-random function} a number of times, treating the number of repetitions as a parameter for computational cost.

One of the early kinds of attacks against password hashes are \emph{rainbow tables}. They are a classic example of a \emph{Time/Memory Tradeoff Attack} (TMTO). Given a dictionary of possible passwords (up to a certain length and using a particular set of symbols) and a hashing algorithm an attacker precomputes hash values well in advance. When a password database of the defender is leaked, the hash values are compared to those from the rainbow table. If two hashes match, the recovery of the original password becomes a matter of a simple lookup.

The currently well-known defence against the rainbow table attack is to use a sufficiently large \emph{salt} when computing the password. This salt is a randomly generated value which is unique for every password and is openly stored alongside it in the defender's database. The salt ensures that the attacker will have to perform the computation after the leak (possibly giving a chance to the defender to change their password). Another nice property is that the same password used by several users will be very likely to produce distinct hash values.

These days an attacker can often compute a large number of hashes in parallel. If so, the increased individual computational time cost of one hash does not prevent the attacker from trying many candidates at once. The throughtput of the attack (average attempted passwords per second) remains high. This type of attacks is addressed by memory-hard password hashing schemes. The main idea is that parallel systems (such as general-purpose \mbox{GPUs} or specialized \mbox{ASICs} and \mbox{FPGAs}) usually have significantly less memory per one processing unit than \mbox{CPUs} of a desktop \mbox{PC}. Therefore, if a password hashing process offers a memory cost parameter, an attacker will need (much) more memory per each specialized processing unit in order to have the same throughtput. This is supposed to make the attack much more expensive or slow it down considerably.

Arguably the first password hashing scheme to implement this idea was scrypt, which was recently published as \mbox{RFC 7914} \cite{percival:2016:scrypt}. However, this scheme is sometimes criticized for being overly complicated and not offering a decoupled way to control time and memory costs. In other words, there is a single parameter that controls both those values.

In order to create new designs which would address the need for a new memory-hard password hashing function, the Password Hashing Competition was announced in 2013 and concluded in 2015 \cite{wetzels:2016:phc}. Below follows a quick summary of its winner (Argon2) and all of the finalists except Lyra2. The latter will be described in more detail later in chapter \ref{chapter:lyra2}.

\subsection{Main Features of \textsc{Catena}}
\label{sec:catena}

\textsc{Catena} is a password-scrambling framework based on Bit Reversal Graphs \cite{forler:2013:catena}. It packs such features as \emph{client-independent updates} which allows a hash value to be updated using larger time or memory cost parameters without the need to wait for a user to login. \textsc{Catena} also offers a \emph{server-relief} feature which allows to offload most of the hash computation to the client machine rather than the server, hence increasing the number of possible concurrent logins.

The \textsc{Catena-Butterfly(-Full)} and \textsc{Catena-Dragonfly(-Full)} are the most notable configurations of \textsc{Catena}. The former one is recommended when the memory-hard property is required. The \textsc{-Full} versions utilize the complete set of rounds of the underlying hash function Blake2b.

An interesting related project is \textsc{Catena-Axungia} which can be found in \cite{github:2017:catena-axungia}. It allows its user to specify the desired time and memory requirements in conventional (for a human) seconds and kilobytes. After that the program returns recommended values which will on average make \textsc{Catena-Butterfly} or \textsc{Catena-Dragonfly} run for the set amount of time and consume the set amount of memory.

Finally, the \textsc{Catena-Variants} \cite{github:2017:catena-variants} project is a modular \texttt{C++} implementation of \textsc{Catena}. It is reported to be slower and consume more memory while at the same time providing more flexibility. There is an API that allows the developer to mix and match internal parts of the algorithm.

\subsection{Main Features of Makwa}
\label{sec:makwa}

The Makwa PHS at its core relies on Blum integers \cite{pornin:2015:makwa}. A Blum integer is a natural integer \(n\) which can be represented as a \(pq\) where \(p\) and \(q\) are prime numbers with an additional property:

\begin{IEEEeqnarray}{rCl}
    p &=& 3 \texttt{ mod } 4 \\
    q &=& 3 \texttt{ mod } 4
\end{IEEEeqnarray}

The squaring is primarily computation intensive and does not require a significant amount of memory. The \(p\) and \(q\) integers should be kept secret, if they are known then the computation can be accelerated considerably.

One of the distinguising features of Makwa highlighted on its website is \emph{delegation} \cite{makwa:2017:homepage}. The author points out that the complexity of password hashing is essentially an arms race between defenders and attackers. Delegation allows to use untrusted systems to perform part of the computation of the hash value with the Makwa algorithm. The exact protocol can be found in Section 4 of the specification \cite{pornin:2015:makwa-spec}.

\subsection{Main Features of yescrypt}
\label{sec:yescrypt}

The \emph{yescrypt} PHS \cite{peslyak:2015:yescrypt} improves upon its predecessor, \emph{scrypt} \cite{percival:2016:scrypt}. However, yescrypt author Alexander Peslyak makes it clear that the author of scrypt is a different person, Colin Percival. The yescrypt PHS deals with some minor inconsistencies discovered in the specification of its predecessor.

The yescrypt PHS also introduces a novel configuration with a read-only memory (ROM) table. In that configuration random lookups are performed so as to ensure that this table remains in memory. Finally, the \texttt{YESCRYPT\_RW} flag enables these lookups as well as a number of optimized instructions.

\subsection{Main Features of Argon2}
\label{sec:argon2}

\emph{Argon2} has two distinct configurations: \emph{Argon2i} and \emph{Argon2d} \cite{biryukov:2015:argon2}. The former revists the blocks of the in-memory matrix in the data-\emph{independent} fashion while the latter does so in a data-\emph{dependent} manner. This means that Argon2i is better suited for scenarios when \emph{side-channel attacks} are a viable concern, such as during password hashing or key derivation. At the same time Argon2d is more resistant to \emph{time-memory tradeoffs} which makes it more suitable for digital cryptocurrencies or other cases where proof of work is important.

Argon2 accepts the following set of parameters: password, salt, degree of paralelism, length of the produced hash (called a \emph{tag} in \cite{biryukov:2015:argon2}), memory cost, time cost, version number (for compatibility reasons, currently at \texttt{0x13}), secret value, associated data and type of configuration to use (Argon2i or Argon2d).

The more interesting parameters are the degree of parallelism as well as secret value together with associated data. The last of these three adds more flexibility to the scheme. The secret value parameter enables \emph{keyed hashing} and improves security in case of a database leak. Keyed hashing is similar to salt but the key is the same for the entire database and is stored in Random Access Memory. This does not introduce theoretical security but instead complicates the technical job for an actual attacker. Finally, the degree of parallelism directly corresponds to the number of rows of the in-memory matrix.

Argon2 is arguably the most widespread and popular alogorithm among all of the finalists. Consequently, the \texttt{README.md} file in the GitHub repository \cite{github:2017:argon2} provides a long list of bindings for various languages. Finally, notable companies like the Django Software Foundation and Yandex are actively using Argon2 in production: \cite{django:2017:argon2, github:2017:argonische}.
