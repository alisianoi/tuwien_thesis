\chapter{Related Work}
\label{chapter:related-work}

\section{Cryptographic Competitions}
\label{sec:cryptocomps}

It is common practice to announce a competition in order to develop standard cryptographic primitives. For example, the symmetric block cipher Rijndael \cite{daemen:2002:DRA} was chosen to become the Advanced Encryption Standard (AES) \cite{aes-fips} in a competitive selection process that lasted from 1997 to 2000. The competition was organized by the National Institute of Standards and Technology (NIST) and included 15 different designs which were narrowed down to 5 during the final phase: Rijndael \cite{daemen:2002:DRA}, Serpent \cite{anderson:1988:serpent}, Twofish \cite{schneier:1998:twofish}, RC6 \cite{rivest:1998:rc6} and MARS \cite{burwick:1998:mars}. Such an open standartization process was highly praised by the cryptographic community. So, another competition was held by NIST between 2007 and 2012 in order to select the next hash function standard, SHA-3. The final round included 5 designs: Blake \cite{aumasson:2013:blake2}, Grøstl \cite{praveen:2011:groestl}, JH \cite{wu:2011:jh}, Skein \cite{ferguson:2009:skein} and Keccak \cite{cryptoeprint:2015:keccak}, the last of which ultimately became the winner of the competition.

Choosing cryptographic primitives through an open competitive process is an effective approach. Therefore, when the need for an updated, memory-hard password hashing scheme became apparent, a Password Hashing Competition was held. This time, however, it was not organized by NIST but directly by the cryptographic community. In particular, the \url{password-hashing.net} website lists two well-known cryptographers Dmitry Khovratovich and Jean-Philippe Aumasson as direct contacts. The Password Hashing Competition concluded in 2015, with Argon2 selected as its winner and Catena, Lyra2, Makwa and yescrypt receiving special recognition.

\section{An Overview of Methods for Secure Authentication}
\label{sec:secure-authentication}

This section will present a brief overview of different methods for secure authentication as well as a short summary of some types of attacks. The presented taxonomy was in part borrowed from \cite{raza:2012:password-attacks-survey} and then expanded and filled with details from other sources.

\subsection{Authentication Approaches}
\label{subsec:authentication-approaches}

Several authentication approaches have emerged over the years. Some of them are much more widespread and old while others are rather novel and aim to solve some known issues. Below is a list of rather common and interesting authentication schemes.

\emph{Conventional Passwords} are by far the most widespread method of user authentication. The service challenges the user to produce a pair of a (public) username and a (private) password. If the provided pair is correct, the user is allowed to proceed, otherwise the request is rejected. Such an intuitive approach works reasonably well but still comes with a number of challenges.

 According to sources from \cite{ives:2004:domino}, a typical user daily accesses roughly 15 services that require a password. At the same time, they actually remember at most 5 unique passwords. The pigenhole principle suggests that some of those passwords are reused for different services. Even if attempts are made to slightly tailor the password to the service, the practice still has a devastating effect on security \cite{gaw2006password, shay2010encountering}. The most protected account becomes as secure as the least protected one and the possibility of a breach grows with the number of services that share the same (or a significantly similar) password.

Advanced password guessing techniques provide interesting insight as well. The authors of \cite{dell:2010:password} use a Markov model to prioritize the order in which an attacker attempts to guess the user's password. The algorithm uses training data to fit a probability distribution over the domain of possible passwords. Once the distribution is constructed, the most probable password candidates are tried first, which in practice noticably increases the speed of password recovery. Intuitively, the best training data is the set of actual passwords themsevles. However, for the password data in \cite{dell:2010:password} it was shown that the second best training data is the set of usernames. This is a peculiar observation (after all, usernames and passwords serve very different purposes) but the provided explanation was that a) both usernames and password must be later recalled by the user and b) upon registration, the user produces both their username and password in quick succession, possibly resuing the same thinking patterns.

Despite their many flaws, conventional passwords remain the backbone of user authentication. At the same time a number of viable alternatives have been developed over the years and are presented below.

\emph{Graphical Passwords} is one such alternative. They can generally be subdivided into two categories: recognition- and recall based \cite{suo:2005:graphical-passwords-survey}. The \emph{recognition based} graphical passwords vary widely, the main idea being that the user has to recognize correct images and perform an action. For example, clicking on previously chosen pictures or clicking inside a convex hull created by them. Pictures of human faces are sometimes suggested because they are arguably the easiest to recall but other types of imagery are used as well. The \emph{recall based} schemes, on the other hand, expect the user to reproduce some type of a drawing. These include the well-known graphical password on smartphones, where the user is typically required to connect points of a \(3 \times 3\) grid with straight lines without removing the finger. More complex approaches either let the user initiate several touch gestures thus producing an even more detailed drawing or write something that resembles a signature.

Some applications of graphical passwords claim better resistance to shoulder surfing attacks \cite{suo:2005:graphical-passwords-survey}. Unfortunately, these schemes usually take longer on average to use when compared to conventional passwords. This adversely affects their practical application.

\emph{Keystroke Dynamics} is an elegant authentication mechanism which tries to capture the typing pattern of a legitimate user. Several statistics about the speed and rhythm with which the user presses the keys are captured: a) the time between consecutive keys b) the time between pressing and releasing the key c) the time it takes to type cetain words etc. A more extensive list of examples can be found in \cite{teh:2013:survey-keystroke-biometrics}. Those statistics are then used in a Machine Learning algorithm which ultimately decides if the user is legitimate or not. These algorithms include fine-tuned \(k\) Nearest Neighbors \cite{ivannikova:2017:anomaly-detection-keystroke-dynamics}, naïve Bayes \cite{ho:2017:onenb}, gaussian mixture models \cite{yunbin:2013:gmm-keystroke} and others.

An immediate challenge is that in this scenario authentication becomes \emph{probabilistic}. Instead of asking "what the user knows", methods based on keystroke dynamics evaluate "what the user is" by judging their ability to type. This becomes particularly difficult in edge cases when the user is under heavy mental stress or is somehow physically restricted. One possible solution is to run authentication as a continuous process \cite{bours2012continuous, serwadda2013scan} and monitor the user over a prolonged period of time. This challenge also appears with a few other biometric authentication methods, of which keystroke dynamics is a prominent example.

\emph{Biometric Authentication} is a collective term for a technique which focuses on physical traits unique to every user. These include fingerprints, facial recognition, voice recognition, iris scanning and similar. Other types of biometric data as well as certain challenges connected with false recognition as well as false non-recognition can be found in \cite{jain:2004:intro-to-biometric}. That paper also highlights the notion of \emph{negative recognition} which means that biometric authentication allows to establish if the person is who they \emph{deny} to be. This property distinguishes these class of methods from all the others described in this section.

\emph{One-time Passwords} is another elegant solution for user authentication. It is usually used in a multi-factor fashion, specifically Two-Factor Authentication (2FA). This means that the user first provides the conventional username/password pair ("what the user knows") and then follows it up with a one time password. This second password can come from a dedicated smart card, a separate keyfile or a smartphone application ("what the user has"). Of major interest are two one time password generation algorithms: HMAC-based One-time Password Algorithm (HOTP \cite{rfc4226}) and Time-based One-time Password Algorithm (TOTP \cite{rfc6238}). They are both developed as part of the Initiative for Open Authentication (OATH). They are implemented by several mobile applications for various operating systems and are also supported in the backend by such companies as Google, Yandex, GitHub, GitLab and many others \cite{wiki:2017:hotp-totp-companies}.

\subsection{Conventional Password Strength}
\label{subsec:password-strength}

Despite the numerous flaws a number of alternative authentication approaches outlined in \ref{subsec:authentication-approaches}, conventional passwords are the primary means of user authentication. This section provides a list of approaches to measure their strength.

\emph{Information Entropy} is the usual way to specify password strength. It is a base-two logarithm of the number of guesses which are required to recover a password with complete certainty. Information entropy is measured in bits. For example, if the password is completely random and has \(x\) bits of information entropy, then an attacker must compute \(2^x\) password candidates to be guaranteed a successful recovery.

Human generated passwords, however, are notorious for generating non-random passwords. Using information entropy to estimate the strength of these passwords produces overly optimistic results. Some alternative approaches are mentioned below.

\emph{NIST Special Publication 800-63-2} \cite{burr2013electronic} tried to address this problem and suggested a refined approach to compute the information entropy of a password:

\begin{itemize}
    \item The entropy of the 1\textsuperscript{st} character is 4 bits.
    \item The entropy of characters 2 to 7 is 2 bits per character.
    \item The entropy of characters 9 to 21 is 1.5 bits per character.
    \item The entropy of characters 21 and above is 1 bit per character.
    \item If \emph{both} uppercase letters and non-alphabetic characters are used, the password gains 6 more bits of entropy.
    \item If the password is 1 to 19 letters long and passes an extensive dictionary check, then an additional 6 bits of entropy should be added.
  \end{itemize}

The last bonus is not assigned to passwords of length 20 and above because it is assumed that those are \emph{pass phrases}, i.e. passwords which consist of several dictionary words. The whole approach was criticized for being inaccurate and has even been removed from the revised version of the publication \cite{grassi2017digital}.

\subsection{Authentication Attacks}
\label{subsec:authentication-attacks}
\emph{Brute Force} is the least sophisticated type of a password attack. The attacker chooses a specific password domain and begins the trial and error guessing process. For example, one could try all passwords that consist of lowercase latin latters (26 in total) and are up to 8 characters in length. This domain contains \(217180147158\) unique passwords. Assuming that the attacker can try \(1000\) passwords per second, it will take them \(\approx 6.8\) years to complete the search. This might sound like a lot of time but the \(1000\) passwords per second is a conservative speed estimate. Depending on the available hardware, the hash algorithm and other factors, this speed could be a \emph{a lot} higher. Real world speeds for some hashing algorithms can be found in table \ref{table:hashcat-speed}

\begin{table}
    \begin{center}
        \begin{tabular}{lr}
            Hashtype & Millions of hashes per second \\
            \hline
            MD5 & 2912.4 \\
            SHA1 & 1004.6 \\
            SHA-512 & 117.6 \\
            SHA-3 (Keccak) & 104.6
          \end{tabular}
      \end{center}
      \caption{Hashcat on a ThinkPad E570 Notebook with an Nvidia GTX 950M graphics card.}
      \label{table:hashcat-speed}
  \end{table}

\emph{Dictionary Attacks} are an improvement upon brute force. A dictionary is a collection of possible passwords, often ordered by popularity or likelihood of occurance. Such dictionaries are compiled from various sources: real dictionaries, texts on websites, previously leaked passwords and so on. Advanced dictionaries also include \emph{mutations} which are a combination of a plain dictionary entry and some modification rule. An example of such modification rule might be a substitution of the letter \(o\) with the digit \(0\) or an addition of \(42\) to the end of the password.

\emph{Shoulder Surfing} is an entirely different kind of an attack. As the name suggests, its simplest form is a person standing next to the user who is performing authentication. However, this is by far not the only scenario. The user might be authenticating from a café, a bank, an airport or any other location with sufficiently advanced video surveillance system. If a high resolution recording of the user's authentication process is captured, then this video becomes a legitimate attack vector.

\emph{Phishing Attacks} are yet another formidable type of an attack. A legitimate user is usually tricked into visiting a webpage which impersonates a typical webservice, like an e-mail provider or a social network. The user is then prompted to enter their authentication details. If they do not recognize the attack in time then their credentials will get compromised. Phishing attacks usually target a large group of individuals which means they cannot use personalized information. On the other hand, \emph{spear phishing attacks} is a term reserved for targeting specific individuals (journalists, politicians, high-profile executives, etc.) in a much more convincing and effective way.

\section{Password Authenticated Key Exchange}

The rest of this work is dedicated to protecting the storage of conventional (short and weak) passwords against offline dictionary attacks. However, such protection would have been completely useless if the communication methods using those passwords were vulnetable to this kind of an attack. \emph{Password Authenticated Key Exchange (PAKE)} is a family of methods which allow to establish secure communications over an insecure channel and combat offline dictionary attacks as well.

\emph{Encrypted Key Exchange (EKE)} was the first method from the PAKE family. It allowed the parties to establish a secure connection with the following properties:

\begin{enumerate}
    \item The attacker cannot subject the conventional password to an offline dictionary attack. Instead, they are forced to attack a much stronger session key.
    \item \emph{Replay attacks} are not possible because the session key is generated by using both a (weak) shared secret as well as a strong random value.
    \item \emph{Man-in-the-middle attacks} are not possible due to the challenge-response nature of the EKE handshake.
  \end{enumerate}

Below is an outline of the first version of this protocol:

\begin{enumerate}
    \item Alice and Bob share a (weak and short) conventional password \(P\).
    \item Alice begins begins by generating a random public/private pair of keys \(E_A\) and \(D_A\). She uses \(P\) to encrypt to public key: \(M_0 = P(E_A)\). She sends \(M_0\) to Bob.
    \item Bob uses \(P\) to decrypt the public key of Alice: \(P^{-1}(P(E_A)) = E_A\). He then generates a random secret key \(R\) and encrypts it first with the asymmetric and then with the symmetric cryptosystem: \(M_1 = P(E_A(R))\). Finally, Bob sends \(M_1\) to Alice.
    \item Alice uses both \(P\) and \(D_A\) to recover \(R\): \(D_A(P^{-1}(P(E_A(R))) = R\).
  \end{enumerate}

Once this initial exchange is over, both Alice and Bob know \(R\) and \(E_A\), which are presumed to be stronger than the initial shared secret \(P\) because they are supposed to be generated randomly over a large possible keyspace. Let's assume that the attacker knows \(M_0 = P(E_A)\), \(M_1 = P(E_A(R))\) and \(R(\text{Known message})\). They then have to try a candidate password \(P'\), produce a candidate \(E_A^{'} = P^{'-1}(P(E_A))\) and then decide if there exists a key \(R'\) such that \(E'_A(R') = E_A(R)\) and \(R^{'-1}(R(\text{Known message}))\) produces the known message. This is expected to be far more expensive than a plain attack on \(P\).

\emph{Password Authenticated Key Exchange by Juggling (J-PAKE)} is a more recent and advanced method from the PAKE family. The connection created by using this protocol has the following properties:

\begin{enumerate}
    \item Resistance to offline dictionary attacks.
    \item Resistance to online dictionary attack: only one password attempt per one session.
    \item Forward secrecy: the session key remains secure if the password becomes known.
    \item Known session key security: the past and future session remain secure even if the current session key becomes known.
  \end{enumerate}

\section{Password Hashing Fundamentals}
\label{sec:fundamentals}

\emph{Password hashing} is the process of transforming a password into a hash value. Given the resulting hash value it should be computationally infeasible to restore the original password.

Password hashing is common practice when user authentication is required. The user provides a password which is then hashed using some password hashing algorithm. The resulting hash is then compared against a hash value which is stored in the database and is known to be correct. If both hashes match, the user is authenticated.

When a password database is leaked, the password hashing process is what prevents the attackers from gaining the original plaintext passwords. One of the early password hashing schemes, which is widely used today, is PBKDF2, described in section 5.2 of \cite{moriarty:2017:pkcs}. The fundamental idea behind it (which is shared among a few other early password hashing schemes) is to apply a \emph{pseudo-random function} a number of times, treating it as a parameter for computational cost.

One of the early kinds of attacks against password hashes are \emph{rainbow tables}. They are a classic example of a \emph{Time/Memory Tradeoff Attack} (TMTO). Given a dictionary of possible passwords (up to a certain length and using a particular set of symbols) and a hashing algorithm an attacker precomputes hash values well in advance. When a password database of the defender is leaked, the hash values are compared to those from the rainbow table and in case of a match the password can be recovered because the computation has already been performed.

The currently well-known practice to combat the rainbow table attack is to use a sufficiently large \emph{salt} when computing the password. This salt is a randomly generated value which is unique for every password and is openly stored alongside it in the defender's database. This ensures that the attacker will have to perform the computation after the leak (possibly giving a chance to the defender to change their password). Another nice property is that the same password used by several users will be very likely to produce distinct hash values.

These days an attacker can often compute a large number of hashes in parallel. If so, the increased individual computational time cost of one hash does not prevent the attacker from trying many combinations at once. The throughtput of the attack (average attempted passwords per second) remains high. This is the type of attacks addressed by memory-hard password hashign schemes. The main idea is that parallel systems (such as general-purpose GPUs or specialized ASICs and FPGAs) usually have significantly less memory per one processing unit. Therefore, if a password hashing process offers a memory cost parameter, an attacker will face greater costs per one parallel computation.

Arguably the first password hashing scheme to implement this idea this was scrypt, which was recently published as RFC 7914 \cite{percival:2016:scrypt}. However, this scheme is sometimes criticized for being overly complicated and not offering a decoupled way to control time and memory costs. In other words, there is a single parameter that controls both those values.

In order to create new designs which would address this problem, the Password Hashing Competition was announced in 2013 and concluded in 2015 \cite{wetzels:2016:phc}. Below follows a quick summary of its winner (Argon2) and all of the finalists except Lyra2. The latter is the main focus of this work and will be described in more detail later in \ref{chapter:lyra2}.

\section{Main Features of \textsc{Catena}}
\label{sec:catena}

\textsc{Catena} is a password-scrambling framework based on Bit Reversal Graphs \cite{forler:2013:catena}. It packs such features as \emph{client-independent updates} which allows a hash value to be updated using larger time or memory cost parameters without the need to wait for a user to login. \textsc{Catena} also offers a \emph{server-relief} feature which allows to offload most of the hash computation to the client machine rather than the server, hence increasing the number of possible concurrent logins.

The \textsc{Catena-Butterfly(-Full)} and \textsc{Catena-Dragonfly(-Full)} are the most notable configurations of \textsc{Catena}. The former one is recommended when the memory-hard property is required. The \textsc{-Full} versions utilize the complete set of rounds of the underlying hash function Blake2b.

An interesting related project is \textsc{Catena-Axungia} which can be found in \cite{github:2017:catena-axungia}. It allows its user to specify the desired time and memory requirements in conventional (for a human) seconds and kilobytes. After that the program returns recommended values which will on average make \textsc{Catena-Butterfly} or \textsc{Catena-Dragonfly} run for the set amount of time and consume the set amount of memory.

Finally, the \textsc{Catena-Variants} \cite{github:2017:catena-variants} project is a modular \texttt{C++} implementation of \textsc{Catena}. It is reported to be slower and consume more memory while at the same time providing more flexibility. There is an API that allows the developer to mix and match internal parts of the algorithm.

\section{Main Features of Makwa}
\label{sec:makwa}

The Makwa PHS at its core relies on Blum integers \cite{pornin:2015:makwa}. A Blum integer is a natural integer \(n\) which can be represented as a \(pq\) where \(p\) and \(q\) are prime numbers with an additional property:

\begin{IEEEeqnarray}{rCl}
    p &=& 3 \texttt{ mod } 4 \\
    q &=& 3 \texttt{ mod } 4
\end{IEEEeqnarray}

The squaring is primarily computation intensive and does not require a significant amount of memory. The \(p\) and \(q\) integers should be kept secret, if they are known then the computation can be accelerated considerably.

One of the distinguising features of Makwa highlighted on its website is \emph{delegation} \cite{makwa:2017:homepage}. The author points out that the complexity of password hashing is essentially an arms race between defenders and attackers. Delegation allows to use untrusted systems to perform part of the computation of the hash value with the Makwa algorithm. The exact protocol can be found in Section 4 of the specification \cite{pornin:2015:makwa-spec}.

\section{Main Features of yescrypt}
\label{sec:yescrypt}

The \emph{yescrypt} PHS \cite{peslyak:2015:yescrypt} improves upon its predecessor, \emph{scrypt} \cite{percival:2016:scrypt}. However, yescrypt author Alexander Peslyak makes it clear that the author of scrypt is a different person, Colin Percival. The yescrypt PHS deals with some minor inconsistencies discovered in the specification of its predecessor.

The yescrypt PHS also introduces a novel configuration with a read-only memory (ROM) table. In that configuration random lookups are performed so as to ensure that this table remains in memory. Finally, the \texttt{YESCRYPT\_RW} flag enables these lookups as well as a number of optimized instructions.

\section{Main Features of Argon2}
\label{sec:argon2}

\emph{Argon2} has two distinct configurations: \emph{Argon2i} and \emph{Argon2d} \cite{biryukov:2015:argon2}. The former revists the blocks of the in-memory matrix in the data-\emph{independent} fashion while the latter does so in a data-\emph{dependent} manner. This means that Argon2i is better suited for scenarios when \emph{side-channel attacks} are a viable concern, such as during password hashing or key derivation. At the same time Argon2d is more resistant to \emph{time-memory tradeoffs} which makes it more suitable for digital cryptocurrencies or other cases where proof of work is important.

Argon2 accepts the following set of parameters: password, salt, degree of paralelism, length of the produced hash (called a \emph{tag} in \cite{biryukov:2015:argon2}), memory cost, time cost, version number (for compatibility reasons, currently at \texttt{0x13}), secret value, associated data and type of configuration to use (Argon2i or Argon2d).

The more interesting parameters are the degree of parallelism as well as secret value together with associated data. The last of these three adds more flexibility to the scheme. The secret value parameter enables \emph{keyed hashing} and improves security in case of a database leak. Keyed hashing is similar to salt but the key is the same for the entire database and is stored in Random Access Memory. This does not introduce theoretical security but instead complicates the technical job for an actual attacker. Finally, the degree of parallelism directly corresponds to the number of rows of the in-memory matrix.

Argon2 is arguably the most widespread and popular alogorithm among all of the finalists. Consequently, the \texttt{README.md} file in the GitHub repository \cite{github:2017:argon2} provides a long list of bindings for various languages. Finally, notable companies like the Django Software Foundation and Yandex are actively using Argon2 in production: \cite{django:2017:argon2, github:2017:argonische}.
